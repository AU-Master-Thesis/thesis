Multi-agent Collaborative Path Planning
Computer Engineering Master Thesis
Kristoï¬€er Plagborg Bak SÃ¸rensen and Jens HÃ¸igaard Jensen
Department of Electrical and Computer Engineering
Aarhus University
Aarhus, Denmark

{201908140, 201907928}@post.au.dk
06-05-2024

Bloop

Supervisor: Andriy Sarabakha
Co-supervisor: Jonas le-Fevre Sejersen
{andriy, jonas.le.fevre}@ece.au.dk

i

Preface
This master thesis is titled â€œMulti-agent Collaborative Path Planningâ€ and is devised by Jens
HÃ¸igaard Jensen and Kristoï¬€er Plagborg Bak SÃ¸rensen. Both authors are students at Aarhus
University, Department of Electrical and Computer Engineering, enrolled in the Computer
Engineering Masterâ€™s programme. Both authors have completed a Bachelorâ€™s degree in Computer Engineering under the same conditions.
The thesis has been conducted in the period from 29-01-2024 to 04-06-2024, and supervised
by Assistant Professor Andriy Sarabakha and co-supervised by PhD Jonas le-Fevre Sejersen.
Enjoy reading,
Jens HÃ¸igaard Jensen & Kristoï¬€er Plagborg Bak SÃ¸rensen

ii

Abstract
This thesis exploresâ€¦ Todo: add more here

iii

Nomenclature
â—† Interior Mutability
â—† Test<T> denotes a type Test which is generic over any type T
â—† Every capital letter in monospace is some type. A type in rust is either a struct, a tagged
enum or a union.

Acronym Index

iv

AABB

Axis Aligned Bounding Box

ADE

Absolute Displacement Error

AOS

Array of Structures

APE

Absolute Position Error

ATE

Absolute Trajectory Error

BP

Belief Propagation

CSR

Compressed Sparse Row

DOF

Degrees of Freedom

ECS

Entity Component System

FDE

Final Displacement Error

GBP

Gaussian Belief Propagation

KL

Kullback-Leibler

LDJ

Log Dimensionless Jerk

MAE

Mean Absolute Error

MAP

Maximum A Posteriori

MAPE

Mean Absolute Position Error

OOP

Object Oriented Programming

RMSE

Root Mean Squared Error

RRT

Rapidly-exploring Random Tree

RRT*

Optimal Rapidly-exploring Random Tree

SIMD

Single Instruction, Multiple Data

SISD

Single Instruction, Single Data

SOA

Structure of Arrays

UAV

Unmanned Aerial Vehicle

v

Contents
Preface

ii

Abstract

iii

Nomenclature

iv

1

2
2
2
3
4
4

Introduction
1.1
1.2
1.3
1.4
1.5

2

Background
2.1
2.2
2.3
2.4
2.5
2.6
2.7
2.8
2.9

3

7
7
7
8
9
11
13
16
18
21
25
25
33
34
36

Study 1: Reproduction
Study 2: Improvements
Study 3: Extension
Study 4: Tooling & Design

Results
4.1
4.2
4.3
4.4
4.5
4.6

5

Related Works
Gaussian Models
Probabilistic Inference
Factor Graphs
Belief Propagation
Gaussian Belief Propagation
Non-Linearities
Rapidly Exploring Random Trees
Optimal Rapidly Exploring Random Trees

Methodology
3.1
3.2
3.3
3.4

4

Motivation
Problem Deï¬nition
Research Hypothesis
Research Questions
Research Objectives

37
37
38
39
39
39
39

Metrics
Scenarios
Study 1: Reproduction
Study 2: Improvements
Study 3: Extension
Study 4: Tooling & Design

Discussion

40

vi

5.1

6

Future Work

40

Conclusion

42

References

43

words

8836

characters

54174

normal pages

22.57

goal pages

100

goal characters

240000

pp./person/day

1.33

days left
22.57%
(22.57
pages)

29
77.43% (77.43 pages)

54.59%
77.17% (98 days)

12 todo

13 Jens

22.83% (29 days)

28 Kristoffer
53 remarks

vii

Kristoï¬€er: make a statement about how we release our software for others to use .e.g license
and terms
Kristoï¬€er: check all libraries we use are in accordance with our terms

1

1

Introduction

1.1

Motivation

Todo: A lot of this is probably in the original contract.
Harnessing the power of automation has been a key driver for the long term development of
human manufacturing and quality of life in the modern world since the industrial revolution
Todo: cite . Along with an increase in automation comes more playroom Todo: wording for
autonomous systems of higher complexity. Many current-day systems, such as the agriculture
industry[1] are on a trajectory toward autonomous multi-agent robotics. This trend can also
be projected onto more mainstream everyday tasks, such as manual transportation. The adoption of autonomous vehicles is increasing, and the technology is becoming more and more
mature. As the fraction of autonomous vehicles increases, the possibilities for eï¬ƒcient traï¬ƒc
expand exponentially with the ability to coordinate and communicate between vehicles. For
such systems to reach their full potential, it is preferred to move at high speeds and ensure
deadlocks are avoided. This is especially relevant in scenarios where the room for movement
is limited, this could be an indoor environment or roads with speciï¬c boundaries. Overall,
putting eï¬€ort towards the development of the multi-agent ï¬eld will help enable many current
and future systems to do more in less time, and with fewer resources.
With this development, the possibility and thus also demand for multi-agent systems have
become much more prevalent. This is especially impacting the ï¬eld of robotics, where separate agents can work together to achieve a common goal, or even work together to achieve
separate goals as eï¬ƒciently as possible.
In the context of multi-agent systems, path planning is a key component. It is a process of
optimization to ï¬nd the most eï¬ƒcient path for an agent to reach its goal. This is especially important in the context of multi-agent systems, where multiple agents need to coordinate their
movements to avoid collisions and reach their goals as eï¬ƒciently as possible. This requires a
high level of collaboration and communication between the agents, as well as a high level of
collision avoidance to ensure that the agents do not collide with each other or with obstacles.

1.2

Problem Deï¬nition

Todo: A lot of this is probably in the original contract.
2

06-05-2024

Chapter 1: Introduction â€• 1.2 Problem Deï¬nition

Eï¬ƒcient and safe approaches to multi-agent planning have been studied extensively, with
many diï¬€erent approaches and assumptions about the environment and capabilities of the
robotic agents. However, none of the existing approaches attempt to be able to operate in
changing communication conditions, in a decentralized manner. Morteza[2], 2021, implements
a modiï¬ed A* algorithm that can handle dynamic obstacles in the environment. They also
propose an architecture where the path planning optimization is done in a centralized manner on a cloud server. This reduces the number of messages sent between the agents but introduces a single point of failure. Xinjie[3], 2023, uses a game-theoretic approach to do path
planning. Here agents cannot communicate with each other and instead have to predict how
other agents will move based on the rules that govern the movement of the agents. Similar
to how humans navigate in vehicle traï¬ƒc, where the driverâ€™s own assumption/belief about
how other drivers will follow the traï¬ƒc law, is used to predict how other vehicles will move.
Aalok[4], 2023, uses Gaussian Belief Propagation to do path planning for multiple agents. It is
a decentralized approach using a data structure called a factor graph to model uncertainties
about the position and velocity of other agents.
The objective of this project is to build on top of the work of [4], and extend it to handle cases with limited communication possibilities, challenging environments, and non-holonomic kinematic constraints. The environment will be a static indoor environment at a logistics facility where robots are integrated into a baggage sorting handling system. Instead of
extending the original paperâ€™s source code, we will reimplement our version of the Gaussian
Belief Propagation Planner algorithm from scratch. The intention of the rewrite is not only to
try and improve performance but also to get a better understanding of the algorithm, to make
it easier to extend it. The implemented solution will be tested in a simulated environment, with
common scenarios found at logistic facilities, such as three-way junctions and intersections.
Todo: Motivations for why our project is interesting
â—† What makes multi robot systems, better/more interesting than single robot systems?
â—† What challenges does it bring with it?
Todo: formulate a strong and clear deï¬nition of what path planning is, and ensure it is consistent with the rest of the document
Kristoï¬€er: Enumerate what makes the Gaussian Belief Propagation algorithm attractive.
â—† Peer to Peer based
â—† No need to synchronize the ordering of messages in time
â—† Modular. New factors can be added to the factor graph to encode diï¬€erent constraints.

1.3

Research Hypothesis

This thesis poses the following 4 hypothesis:
H-1 Reproducing the results of the original GBP Planner in a new programming language
will improve the softwareâ€™s scientiï¬c communication and its extensibility.
H-2 Improvements can be made to the original work without transforming the software.
3

Chapter 1: Introduction â€• 1.3 Research Hypothesis

06-05-2024

H-3 Extending the original GBP Planner software with a global planning layer will extend
the actorsâ€™ capability to move in complex environments, without degradation to the reproduced local cooperative collision avoidance, while maintaining a competitive level
of performance.
Layout
How to
measure,
frame
time, fps

H-4 Extensive tooling will create a great environment for others to understand the software
and extend it further. Furthermore, such tooling will make it easier to reproduce and
engage with the developed solution software.
From this point on, anything pertaining to the context of Hypothesis H-1 will be referred
to as Study 1: Reproduction, Hypothesis H-2 will be Study 2: Improvements, Hypothesis
H-3 will be Study 3: Extension, and Hypothesis H-4 will be Study 4: Tooling & Design.

1.4

Research Questions

Study 1 â€” Questions for hypothesis H-1 :
RQ-1.1 Which programming language will be optimal for scientiï¬c communication and extensibility?
RQ-1.2 Is it possible to reproduce the results of the original GBP Planner in the chosen programming language?

Wordings too
harsh?

Study 2 â€” Questions for hypothesis H-2 :
RQ-2.1 Which immediate improvements are obivous from looking at the original work?
RQ-2.2 Are these improvements possible without transforming the software?
RQ-2.3 Can these improvements be measured, and if so, how?
Study 3 â€” Questions for hypothesis H-3 :
RQ-3.1 Will global planning improve the actorsâ€™ capability to move in complex environments?
RQ-3.2 Will global planning degrade the reproduced local cooperative collision avoidance?
RQ-3.3 Will the global planning maintain a competitive level of performance?
Study 4 â€” Questions for hypothesis H-4 :
RQ-4.1 What kind of tooling will be most beneï¬cial for the software?
RQ-4.2 How can tooling help with future reproducibility and engagement with the software? Todo: maybe chance these or rephrase
RQ-4.3 How can tooling help with understanding and extending the software?
Todo: maybe chance these or rephrase

1.5

Research Objectives

Study 1: Reproduction
Objectives for Research Question RQ-1.1 :

4

06-05-2024

Chapter 1: Introduction â€• 1.5 Research Objectives

O-1.1.1 Evaluate possible programming languages on several metrics for scientiï¬c communication and extensibility. Todo: rephrase, or mention diï¬€erent metrics.
Layout
Learn by
reproducing

Objectives for Research Question RQ-1.2 :
O-1.2.1 Reimplement the original GBP Planner in the chosen programming language.
O-1.2.2 Evaluate whether the reimplementation is faithful to the original GBP Planner by
comparing the four metrics: distance travelled, makespan, smoothness, and collision
count.

Study 2: Improvements
Objectives for Research Question RQ-2.1 :
O-2.1.1 Identify immediate improvements to the original work.
Objectives for Research Question RQ-2.2 :
O-2.2.1 Implement the identiï¬ed improvements without transforming the software.
O-2.2.2 Evaluate whether the improvements are transformative by comparing the four metrics: distance travelled, makespan, smoothness, and collision count.
Objectives for Research Question RQ-2.3 :
O-2.3.1 Use the measurements from O-2.2.2 to evaluate the improvements.

Study 3: Extension
Objectives for Research Question RQ-3.1 :
O-3.1.1 Implement a global planning layer in the reimplemented GBP Planner.
O-3.1.2 Evaluate the actorsâ€™ capability to move in complex environments by looking at the
four metrics: distance travelled, makespan, smoothness, and collision count, comparing against the reimplemented reproduction and the original GBP Planner.
Objectives for Research Question RQ-3.2 :
O-3.2.1 Compare the four metrics: distance travelled, makespan, smoothness, and collision
count of the reimplemented reproduction with and without the global planning
layer.
Objectives for Research Question RQ-3.3 :
O-3.3.1 Compare performance metrics of the reimplemented reproduction with and without
the global planning layer against the original GBP Planner.

Study 4: Tooling & Design
Objectives for Research Question RQ-4.1 :
O-4.1.1 Analyse and evaluate diï¬€erent kinds of tooling that can be beneï¬cial for the software.
O-4.1.2 Pick the most beneï¬cial tooling approach for the software.
Objectives for Research Question RQ-4.2 :
O-4.2.1 Implement tooling to help with future reproducibility and engagement with the software. Todo: maybe chance these or rephrase
Objectives for Research Question RQ-4.3 :

5

Chapter 1: Introduction â€• 1.5 Research Objectives

06-05-2024

O-4.3.1 Implement tooling to help with understanding and extending the software.
Todo: maybe chance these or rephrase
Todo: Part of the argument for H-2: Furthermore, a language with that shares qualities
Kristoï¬€er: wording with modelling languages will improve the softwareâ€™s ability to communicate scientiï¬c results.
Todo: argument for rust: Half way a modelling language, which is optimal for scientiï¬c
communication and extensibility.
Jens: make ï¬gure that shows the connection of all these, including outlining which parts
are which study.

6

2

Background

Firstly, background knowledge on related works is presented, in 2.1, followed by a technical
introduction to the underlying theory for this thesis. In Section 2.2 and 2.3, Gaussian models
and probabilistic inference are introduced, respectively. These two topics are the theoretical
base for understanding factor graphs and the corresponding methods for inference and reasoning about them, as detailed in sections 2.4-2.6.

2.1

Related Works

Originally Murai et al. showed, with their A Robot Web for Distributed Many-Device Localisation, that Gaussian Belief Propagation (GBP) can be utilised to solve multi-agent localisation.
Then Patwardhan et al. showed that the same algorithm structure can be adapted to multiagent planning. This thesis aims to extend the work by Patwardhan et al.[5] to include a global
planning layer, which can provide a more robust solution to the multi-agent planning in highly
complex environments. As such [5] will be covered more thoroughly in its own dedicated
section below, see Section 2.1.1.
Todo: maybe the â€˜dedicated sectionâ€™ is simply the rest of the background which contains the
relevant theory.

2.1.1

GBP Planner

2.2

Gaussian Models

To eventually understand GBP, the underlying theory of Gaussian models is detailed in this
section. Gaussian models are often chosen when representing uncertainty due to the following reasons:
Reason 1 Realistic Modeling: Gaussian models eï¬€ectively capture the way many physical
phenomena and sensor readings are distributed in the real world.[6]
Reason 2 Mathematical Simplicity: Gaussian models have a clean mathematical structure, making them easy to work with.[6]
7

Chapter 2: Background â€• 2.2 Gaussian Models

06-05-2024

Reason 3 Computational Efï¬ciency: Calculations involving Gaussian models can be performed using straightforward formulas, keeping computations fast.[6]
Reason 4 Flexibility: Gaussian models maintain their form under common statistical operations (marginalization, conditioning, products), ensuring ease of manipulation
within robotic systems.[6]
A Gaussian distribution can be represented in the exponential energy form, which is a common way of representing probability distributions. The exponential energy form is deï¬ned as
in (1)[6]:
ğ‘(ğ‘¥) =

1
exp(âˆ’ğ¸(ğ‘¥))
ğ‘

(1)

Jens: What does ğ‘ and ğ¸ mean?
In the exponential energy form, a Gaussian model can be represented in two ways; the
moments form and the canonical form. The moments form is deï¬ned by the mean vector, ğœ‡,
and the covariance matrix, Î£.[6] The canonical form is deï¬ned by the information vector, ğœ‚,
and the precision matrix, Î›. The energy parameters, energy equations, and computational efï¬ciency for certain aspects of the two forms are compared in Figure 1.
Moments Form

Canonical Form

Parameters:

Parameters:

Mean:

ğœ‡

Information:

ğœ‚ = Î£âˆ’1 ğœ‡

Covariance:

Î£

Precision:

Î› = Î£âˆ’1

Energy Equation:

Energy Equation:

ğ¸(ğ‘¥) = 12 (ğ‘¥ âˆ’ ğœ‡)âŠ¤ Î£âˆ’1 (ğ‘¥ âˆ’ ğœ‡)

ğ¸(ğ‘¥) = 12 ğ‘¥âŠ¤ Î›ğ‘¥ âˆ’ ğœ‚âŠ¤ ğ‘¥

Computational Eï¬ƒciency:

Computational Eï¬ƒciency:

Marginalisation:

Cheap

Marginalisation:

Expensive

Conditioning:

Expensive

Conditioning:

Cheap

Product:

Expensive

Product:

Cheap

Figure 1. Camparison between the moments form and the canonical form of representing Gaussian models.[6]
Jonas
Is
it necessary to say
â€œFigure
fromâ€

As outlined in Figure 1 the Canonical Form is much more computationally eï¬ƒcient when it
comes to conditioning and taking products, while the Moments Form excels at marginalisation.
[6] Jens: explain why this is the case

2.3

Probabilistic Inference

To contextualise factor graph inference, the underlying probabilistic inference theory is introduced. The goal of probabilistic inference is to estimate the probability distribution of a
8

06-05-2024

Chapter 2: Background â€• 2.3 Probabilistic Inference

set of unknown variables, ğ‘‹, given some observed or known quantities, ğ·. This is done by
combining prior knowledge with ğ·, to infer the most likely distribution of the variables.[6]
See Example 1.

Example 1: Probabilistic Inference in Meteorology
An everyday example of probabilistic inference is in the ï¬eld of meteorology. Meteorologists use prior knowledge of weather patterns (ğ·), combined with observed data to infer
the most likely weather forecast for the upcoming days (ğ‘‹).
Bayeâ€™s rule is the foundation of probabilistic inference, and is used to update the probability
distribution of a set of variables, ğ‘‹, given some observed data, ğ·. The rule is deï¬ned as in
(2)[6]:
ğ‘(ğ‘‹|ğ·) =

ğ‘(ğ·|ğ‘‹)ğ‘(ğ‘‹)
ğ‘(ğ·)

(2)

This posterior distribution describes our belief of ğ‘‹, after observing ğ·, which can then be used
for decision making about possible future states.[6] Furthermore, when we have the posterior,
properties about ğ‘‹ can be computed;
Property 1 The most likely state of ğ‘‹, the Maximum A Posteriori (MAP) estimate ğ‘‹MAP , is
the state with the highest probability in the posterior distribution. See (3)[6]:
ğ‘‹MAP = argmaxğ‘‹ ğ‘(ğ‘‹|ğ·)

(3)

Property 2 The marginal posteriors, summarising our beliefs of individual variables in ğ‘‹,
can be computed by marginalising the posterior distribution, see (4)[6]:
ğ‘(ğ‘‹ğ‘– |ğ·) = âˆ‘ ğ‘(ğ‘‹|ğ·)
ğ‘‹\ğ‘¥ğ‘–

2.4

(4)

Factor Graphs

A factor graph is a bipartite graph, where the nodes are divided into two disjoint sets; variables and factors. And exempliï¬cation of a factor graph and important intuition is shown in
Example 2. The edges between nodes each connect one from each set, and represent the dependencies between the variables and factors. A factor graph represents the factorisation of
any positive joint distribution , ğ‘(ğ‘‹), as stated by the Hammersley-Cliï¬€ord Theorom. That is,
a product of factors for each clique of variables in the graph, which can be seen in (5)[5â€“9].
ğ‘(ğ‘‹) = âˆ ğ‘“ğ‘– (ğ‘‹ğ‘– )
{ğ‘–}

9

(5)

Chapter 2: Background â€• 2.4 Factor Graphs

06-05-2024

Thus interpreting this, the factors are not necessarily in themselves probabilities, but rather
the functions that determine the probabilities of the variables.[8,9] Additionally, it can be useful to present factor graphs as energy-based models[10], where, as seen in (6)[6], each factor
ğ‘“ğ‘– is associated with an energy ğ¸ğ‘– > 0:[6]
ğ‘“ğ‘–(ğ‘‹ğ‘– ) = exp(âˆ’ğ¸ğ‘–(ğ‘‹ğ‘– ) )

(6)

This presentation also gives another way of ï¬nding the MAP estimate, by ï¬nding the state
with the lowest energy in the factor graph, see (7)[6]:
ğ‘‹MAP = arg minğ‘‹ âˆ’ log ğ‘(ğ‘‹)
= arg minğ‘‹ âˆ‘ ğ¸ğ‘–(ğ‘‹ğ‘– )

(7)

ğ‘–

Example 2
An example factor graph is visualised in Figure 2, with variables {ğ‘£1 , â€¦, ğ‘£4 } and factors
{ğ‘“1 , â€¦, ğ‘“4 }. Writing out the visualised factor graph produces:
ğ‘(ğ‘£1 , ğ‘£2 , ğ‘£3 , ğ‘£4 ) =
Wordings

1
ğ‘“ (ğ‘£ , ğ‘£ , ğ‘£ )ğ‘“ (ğ‘£ , ğ‘£ )ğ‘“ (ğ‘£ , ğ‘£ )ğ‘“ (ğ‘£ )
ğ‘ 1 1 2 3 2 3 4 3 3 4 4 4

(8)

Should
this
be
here?
gbpvisual-introduction

doesnâ€™t
have it
Figure 2. A factor graph is a bipartite graph, where the nodes are divided into two sets; variables and factors.
Variables are represented as red circles , and factors as blue squares . The edges between the nodes represent
the dependencies between the variables and factors.

The factor graph is a generalisation of constraint graphs, and can represent any join function.
Moreover, the factor graph structure enables eï¬ƒcient computation of marginal distributions
through the sum-product algorithm.[8,9] The sum-product algorithm is detailed in Section 2.5.

10

06-05-2024

Chapter 2: Background â€• 2.4 Factor Graphs

Figure 3. Here shown are two factor graphs, one for a green robot, and one for a purple robot. In this speciï¬c case
the two robots are close to each other, and perfectly aligned. At the top, the planning horizon is shown in red , ğ‘›
times-steps into the future, {ğ‘¡1 , ğ‘¡2 , â€¦, ğ‘¡ğ‘› }. Variables are visualised as circles, and factors as squares.

Wordings ğ‘£
1

Wordings

In Figure 3 two joint factor graphs are visualised. The ï¬rst variables in each factor graph
ğ‘£1 , represent the location of a green and purple robot respectively. Each robot has a corresponding factorgraph, where the ï¬gure shows how the two factor graphs are connected with
interrobot factors ğ‘“ğ‘– when they are close enough to each other. Variables ğ‘£2 , â€¦, ğ‘£ğ‘› represent
the future predicted states of the robot respectively at timesteps ğ‘¡2 , â€¦, ğ‘¡ğ‘› , where ğ‘¡1 is the current time.

planned

2.5
Wordings

Belief Propagation

The process of performing inference on a factor graph is done by passing messages between
the variables and factors. Figure 4 visualises the two major steps; Variable Iteration and Factor
Iteration, each with two sub-steps; an internal update, and a message passing step.
Step 1 Variable update
Step 2 Variable to factor message

Step 3 Factor update
Step 4 Factor to variable message

Figure 4. The four steps of propagating messages in a factor graph. Firstly, the variables are internally updated, and
new messages are sent to neighbouring factors , who then update internally, sending the updated messaages back
to neighbouring variables .

11

Chapter 2: Background â€• 2.5 Belief Propagation

06-05-2024

Note that this ï¬gure shows the Variable Iteration ï¬rst, however, performing the Factor Iteration ï¬rst is also a valid, the
main idea is simply that they are alternating.

Jens: context for BP and the sum-product algorithm
In Step 1 the computation of the marginal distribution of a variable ğ‘¥ğ‘– takes place. This
is done by ï¬nding the product of all messages from neighbouring factors ğ‘“ğ‘— to ğ‘¥ğ‘– , as seen in
(9)[5,6,11].
ğ‘šğ‘¥ğ‘– = âˆ ğ‘šğ‘“ğ‘  â†’ğ‘¥ğ‘–

(9)

ğ‘ âˆˆğ‘(ğ‘–)

Secondly, in Step 2 the variable to factor messages ğ‘šğ‘¥ğ‘– â†’ğ‘“ğ‘— are computed as described in (10)
[5], which is a product of all messages from neighbouring factors ğ‘“ğ‘  except ğ‘“ğ‘— .[5,6,11]
ğ‘šğ‘¥ğ‘– â†’ğ‘“ğ‘— =

âˆ

ğ‘šğ‘“ğ‘  â†’ğ‘¥ğ‘–

ğ‘ âˆˆğ‘(ğ‘–)\ğ‘—

(10)

The factor to variable messages ğ‘šğ‘“ğ‘— â†’ğ‘¥ğ‘– are described in (11)[5], where the message is the
product of the factor ğ‘“ğ‘— and the messages from all neighbouring variables ğ‘¥ğ‘– except ğ‘¥ğ‘– itself.
[5,6,11] This corresponds to the entire Factor Iteration, i.e. Step 3 and Step 4 , also shown
in Figure 4.
ğ‘šğ‘“ğ‘— â†’ğ‘¥ğ‘– = âˆ‘ ğ‘“ğ‘— (ğ‘‹ğ‘— )
ğ‘‹ğ‘— \ğ‘¥ğ‘–

âˆ
ğ‘˜âˆˆğ‘(ğ‘—)\ğ‘–

ğ‘šğ‘¥ğ‘˜ â†’ğ‘“ğ‘—

(11)

Jens: ï¬nish this section
Originally Belief Propagation (BP), was created for inference in trees, where each message
passing iteration is synchronous. This is a simpler environment to guarantee convergence in,
and in fact after one synchronous message sweep from root to leaves, exact marginals would
be calculated. However, factor graphs, as explained earlier, are not necessarily trees; they can
contain cycles, and as such loopy BP is required. Loopy BP, instead of sweeping messages,
applies the message passing steps to each each at every iteration, but still in a synchronous
fashion.[6] Jens: more citation for loopy BP
The expansion to loopy graphs is not without its challenges, as the convergence of the
algorithm is not guaranteed. As such the problem transforms from an exact method to and
approximation. This means, that instead of minimising the factor energies through MAP directly, loopy BP minimises the Kullback-Leibler (KL) divergence between the true distribution
and the approximated distribution, which can then be used as a proxy for marginals after satisfactory optimisation.[6]
Loopy BP is derived via the Bethe free energy, which is a constrained minimisation of an
approximation of the KL divergence. As the Bethe free energy is non-convex, the algorithm
isnâ€™t guaranteed to converge, and furthermore, it might converge to local minima in some
cases. It has been shown that empirically loppy BP is very capable of converging to the true
marginals, as long as the graphs arenâ€™t highly cyclic.[6]
Wordings too
loopy?
Is loopy
and cyclic
the same
thing?

12

06-05-2024

2.6

Chapter 2: Background

Gaussian Belief Propagation

Jens: do this ğŸ˜„
Having introduced both Gaussian models, and BP, we can now take a look at GBP. GBP is
a variant of BP, where, due to the closure properties Jens: cite of Gaussians, the messages
and beliefs are represented by Gaussian distributions. In its base form GBP works by passing
Gaussians around in the Canonical Form, i.e. the messages and beliefs contain the precision
matrix, Î›, and the information vector ğœ‚. As mentioned earlier, general BP is not guaranteed to
compute exact marginals, however, for GBP exact marginal means are guaranteed, and even
though the variances often converge to the true marginals, there exists no such guaranteed.[6]
In a factor graph, where all factors are Gaussian, and since all energy terms are additive in
the Canonical Form, the energy of the factor graph is also Gaussian, which means that one
can represent it as a single multivariate Gaussian. See the equation for this joint distribution
in (12)[6]:
1
ğ‘(ğ‘‹) âˆ exp(âˆ’ ğ‘‹ âŠ¤ Î›ğ‘‹ + ğœ‚âŠ¤ ğ‘‹)
2

(12)

2.6.1.1 MAP Inference
In the context of GBP, the MAP estimate can be found by the parameters ğ‘‹MAP that maximises
the joint distribution in (12). The total energy can then be written as (13)[6]
âˆ‡ğ‘‹ ğ¸(ğ‘‹) = âˆ‡ğ‘‹ log ğ‘ƒ (ğ‘‹) = âˆ’Î›ğ‘‹ + ğœ‚

(13)

which is the gradient of the log-probability, and can be set to zero, âˆ‡ğ‘‹ ğ¸ = 0, to ï¬nd the MAP
estimate, which, in GBP is reduced to the mean ğœ‡, as seen in (14)[6]:
ğ‘‹MAP = Î›âˆ’1 ğœ‚ = ğœ‡

(14)

2.6.1.2 Marginal Inference
The goal of marginal inference in GBP is to ï¬nd the per variable marginal posterior distributions. In the Moments Form this looks like (15)[6]:
ğ‘(ğ‘¥ğ‘– ) = âˆ« ğ‘(ğ‘‹) ğ‘‘ğ‘‹âˆ’ğ‘–
1
âˆ exp(âˆ’ (ğ‘¥ğ‘– âˆ’ ğœ‡ğ‘– )âŠ¤ Î£âˆ’1
ğ‘–ğ‘– (ğ‘¥ğ‘– âˆ’ ğœ‡ğ‘– ))
2

(15)

where ğ‘‹{âˆ’ğ‘–} is the set of all variables except ğ‘¥ğ‘– , and Î£ğ‘–ğ‘– is the ğ‘–th diagonal element of the
covariance matrix Î£ = Î›. The marginal posterior distribution is then a Gaussian with mean
ğœ‡ğ‘– and variance Î£ğ‘–ğ‘– . Furthermore, ğœ‡ğ‘– is the ğ‘–th element of the joint mean vector ğœ‡, as in (12).
With the understanding from 2.6.1.1 and 2.6.1.2 inference in GBP ends up being a matter of
solving the linear system of equations (16)[6]:
ğ´ğ‘¥ = ğ‘ â‡’ Î›ğœ‡ = ğœ‚
13

(16)

Chapter 2: Background â€• 2.6 Gaussian Belief Propagation

06-05-2024

Where MAP inference solves for the mean, ğœ‡, and marginal inference ï¬nds the covariance, Î£,
by solving for the block diagonal of Î›âˆ’1 , and indirectly also the mean, ğœ‡.

2.6.2

Variable Update

The variable belief update happens by taken the product of incoming messages from nerighbouring nodes, here denoted as ğ‘ (ğ‘–), as seen in (17)[6]:
ğ‘ğ‘–(ğ‘¥ğ‘– ) = âˆ ğ‘šğ‘“ğ‘  â†’ğ‘¥ğ‘–

(17)

ğ‘ âˆˆğ‘(ğ‘–)

Writing out the Gaussian message on Canonical Form becomes (18)[6]:
1
ğ‘š = ğ’©âˆ’1 (ğ‘¥; ğœ‡, Î›) âˆ exp(âˆ’ ğ‘¥âŠ¤ Î›ğ‘¥ + ğœ‚âŠ¤ ğ‘¥)
2

(18)

Fortunately, as the messages are stored on Canonical Form, the product in (17) is the same as
summing up the information vectors and precision matrices, as seen in (19)[6]:
ğœ‚ğ‘ğ‘– = âˆ‘ ğœ‚ğ‘“ğ‘  â†’ğ‘¥ğ‘–

and

Î›ğ‘ğ‘– = âˆ‘ Î›ğ‘“ğ‘  â†’ğ‘¥ğ‘–

ğ‘ âˆˆğ‘(ğ‘–)

2.6.3

ğ‘ âˆˆğ‘(ğ‘–)

(19)

Variable to Factor Message

The variable to factor message passing is described in (20)[6], where the message is the product of the incoming messages from all neighbouring factors ğ‘“ğ‘— except the factor ğ‘“ğ‘– itself, same
as described in (10)[5]:
ğ‘šğ‘¥ğ‘– â†’ğ‘“ğ‘— =

âˆ

ğ‘šğ‘“ğ‘  â†’ğ‘¥ğ‘–

(20)

ğ‘ âˆˆğ‘(ğ‘–)\ğ‘—

Again in this case, the message is sent in the Canonical Form form, and as such the outgoing
messages can simply be computed by summing up the information vectors and precision matrices, as seen in (21)[6]:
ğœ‚ğ‘¥ğ‘– â†’ğ‘“ğ‘— =

âˆ‘ ğœ‚ğ‘“ğ‘  â†’ğ‘¥ğ‘–

and

ğ‘ âˆˆğ‘(ğ‘–)\ğ‘—

Î›ğ‘¥ğ‘– â†’ğ‘“ğ‘— =

âˆ‘ Î›ğ‘“ğ‘  â†’ğ‘¥ğ‘–
ğ‘ âˆˆğ‘(ğ‘–)\ğ‘—

14

(21)

06-05-2024

2.6.4

Chapter 2: Background â€• 2.6 Gaussian Belief Propagation

Factor Update

Jens: describe how factor distance is marginalised and factors are updated
1. Update linearisation point
2. Measurement & jacobian around linearisation point The measurement residual is
ğ‘šğ‘Ÿ = ğ‘š(ğ‘‹0 ) âˆ’ ğ‘š(ğ‘‹ğ‘› )

(22)

Where ğ‘‹0 is the conï¬guration at ğ‘¡0 , and ğ‘‹ğ‘› is the conï¬guration at the current timestep
ğ‘¡ğ‘› .
3. Factor potential update
Î›ğ‘ = ğ‰âŠ¤ Î›ğ‘€ ğ‰
ğœ‚ğ‘ = ğ‰âŠ¤ Î›ğ‘€ (ğ‰ğ‘™ğ‘ + ğ‘šğ‘Ÿ )

(23)

Where Î›ğ‘ and ğœ‚ğ‘ denotes the precision matrix and information vector of the factor potential, and Î›ğ‘€ is the measurement precision matrix.
4. Factor marginalisation

2.6.5

Factor to Variable Message Passing

Before marginalising, messages from nerighbouring variables are aggregated into a single
message, as seen in (24)[6]:
ğ‘šğ‘“ğ‘– â†’ğ‘¥ğ‘— =

âˆ

ğ‘šğ‘¥ğ‘  â†’ğ‘“ğ‘–

ğ‘ âˆˆğ‘(ğ‘—)\ğ‘–

(24)

Example 3
Consider a factor ğ‘“ connected to 3 variables; ğ‘¥1 , ğ‘¥2 , ğ‘¥3 , and we want to compute the message to be passed to variable ğ‘¥1 . Write the factor out as a Gaussian distribution, see (25)[6]:
ğ‘“([ğ‘¥1 ğ‘¥2 ğ‘¥3 ]) = ğ’©âˆ’1 ([ğ‘¥1 ğ‘¥2 ğ‘¥3 ]; ğœ‚ğ‘“ , Î›ğ‘“ )

(25)

Here, the two Gaussian parameters ğœ‚ğ‘“ and Î›ğ‘“ can be expanded to see the individual contributions from each variable, as seen in (26)[6]:
ğœ‚ğ‘“1
â¡ğœ‚ â¤
ğœ‚ğ‘“ = â¢ ğ‘“2 â¥
ğœ‚
â£ ğ‘“3 â¦

and

Î›
Î›
Î›
â¡ ğ‘“11 ğ‘“12 ğ‘“13 â¤
Î›ğ‘“ = â¢Î›ğ‘“21 Î›ğ‘“22 Î›ğ‘“23 â¥
â¢
â¥
Î›ğ‘“31 Î›ğ‘“32 Î›ğ‘“33
â£
â¦

15

(26)

Chapter 2: Background â€• 2.6 Gaussian Belief Propagation

06-05-2024

As described earlier, ï¬rstly step is to compute the message to be passed to ğ‘¥1 , which is
the product of the incoming messages from ğ‘¥2 and ğ‘¥3 , as seen in (27), as we are on the
Canonical Form this is a summation, and yields the Gaussian, ğ’©(ğœ‚ğ‘“â€² , Î›â€²ğ‘“ )[6]:
ğœ‚ğ‘“1
â¡
â¤
ğœ‚ğ‘“â€² = â¢ğœ‚ğ‘“2 + ğœ‚ğ‘¥2 â†’ğ‘“ â¥
â¢
â¥
ğœ‚ğ‘“3 + ğœ‚ğ‘¥3 â†’ğ‘“
â£
â¦

and

Î›
Î›ğ‘“12
Î›ğ‘“13
â¡ ğ‘“11
â¤
â¥ (27)
Î›ğ‘“23
Î›â€²ğ‘“ = â¢Î›ğ‘“21 Î›ğ‘“22 + Î›ğ‘¥2 â†’ğ‘“
â¢
â¥
Î›ğ‘“31
Î›ğ‘“32
Î›ğ‘“33 + Î›ğ‘¥3 â†’ğ‘“
â£
â¦

Now as we are passing a message to ğ‘¥1 , we have to marginalise out all other variables,
ğ‘¥2 and ğ‘¥3 . This is done by the marginalisation equations given by [12] for Gaussians in
Canonical Form. See (28) and (29) for the joint distribution over variables ğ‘ and ğ‘[6,12].
ğœ‚ğ‘
ğœ‚ = [ğœ‚ ]

Î›ğ‘ğ‘ Î›ğ‘ğ‘
]
Î›ğ‘ğ‘ Î›ğ‘ğ‘

(28)

and Î›ğ‘ğ‘ = Î›ğ‘€ğ‘ âˆ’ Î›ğ‘ğ‘ Î›âˆ’1
ğ‘ğ‘ Î›ğ‘ğ‘

(29)

and

ğ‘

ğœ‚ğ‘€ğ‘ = ğœ‚ğ‘ + Î›ğ‘ğ‘ Î›âˆ’1
ğ‘ğ‘ ğœ‚ğ‘

Î›=[

Now to marginalise, perform the two steps:
Step 1: Reorder the vector ğœ‚ğ‘“â€² and the matrix Î›â€²ğ‘“ to bring the contribution from
the recipient ğ‘¥1 to the top.
In our case no reordering is to be done, as ğ‘¥1 is already at the top.
Jens: maybe an example where reordering is necessary is better
Step 2: Recognise the subblocks ğ‘ and ğ‘ from (28) and (29).
In our case ğ‘ = ğ‘¥1 and ğ‘ = [ğ‘¥2 ğ‘¥3 ].

2.7

Non-Linearities

Jens: and this
Non-linear factors can exist, however, to understand the impact, letâ€™s ï¬rst look at linear
factors. A factor is usually modeled with data ğ‘‘. Equation (30)[6] shows how this is written:
ğ‘‘ âˆ¼ ğ‘š(ğ‘‹ğ‘› ) + ğœ–

(30)

Here, ğ‘š(ğ‘‹ğ‘› ) represents the measurement of the state of the subset of neighbouring variables,
ğ‘‹ğ‘› , to the factor, and the error term ğœ– âˆ¼ ğ’©(0, Î£ğ‘› ) is white noise. Thus, ï¬nding the residual ğ‘Ÿ
between the measurement and the model, as seen in (31)[6], reveals propagates the Gaussian
nature of the model to the residual.
ğ‘Ÿ = ğ‘‘ âˆ’ ğ‘š(ğ‘‹ğ‘› ) âˆ¼ ğ’©(0, Î£ğ‘› )

(31)

With this, looking at Figure 1, the Moments Form can be rewritten with the measurement,
ğ‘š(ğ‘¥), and the model ğ‘‘ (32)[6]:
16

06-05-2024

Chapter 2: Background â€• 2.7 Non-Linearities

1
ğ¸(ğ‘‹ğ‘› ) = (ğ‘š(ğ‘‹ğ‘› ) âˆ’ ğ‘‘)âŠ¤ Î£ğ‘› âˆ’1 (ğ‘š(ğ‘‹ğ‘› ) âˆ’ ğ‘‘)
2

(32)

In case of a linear factor, the measurement function is quadratic and can be written as
ğ‘š(ğ‘‹ğ‘› ) = ğ‰ğ‘‹ğ‘› + ğ‘, where ğ‰ is the jacobian. This allows us to rearrange the energy onto
Canonical Form (33)[6]:
1
ğ¸(ğ‘‹ğ‘› ) = ğ‘‹ğ‘›âŠ¤ Î›ğ‘‹ğ‘› âˆ’ ğœ‚âŠ¤ ğ‘‹ğ‘›
2

, where ğœ‚ = ğ‰âŠ¤ Î£ğ‘› âˆ’1 (ğ‘‘ âˆ’ ğ‘) and Î› = ğ‰âŠ¤ Î£ğ‘› âˆ’1 ğ‰ (33)

However, in case of a non-linearity in ğ‘š, the energy is also not quadratic in ğ‘‹ğ‘› , which in turn
means that the factor is not Gaussian. To achieve a Gaussian distribution for the factor in this
case, it is necessary to linearise around a current estimate ğ‘‹0 , which is from here called the
linearisation point. This linearisation takes place by (34)[6]:
ğ‘š(ğ‘‹ğ‘› ) = ğ‘š(ğ‘‹0 ) + ğ‰(ğ‘‹ğ‘› âˆ’ ğ‘‹0 )

(34)

As such, we end up with a linearised factor on the form (35)[6], which ends up with a Gaussian
approximation of the true non-linear distribution:
ğ‘ = ğ‘š(ğ‘‹ğ‘› ) âˆ’ ğ‰ğ‘‹ğ‘›

(35)

The underlying potential non-linearities of factors is exempliï¬ed in Example 4, and visualised
in Figure 5.

Example 4
Jens: make this example with similar ï¬gure as to [6]

Figure 5. A non-linear factor is visualised, where the measurement function ğ‘š(ğ‘‹ğ‘› ) is non-linear. The linearisation point ğ‘™0 is shown, and the robotâ€™s position . The non-linear true distribution is visualised as a grey
contour plot underneat the linearised gaussian distribution on top.

17

Chapter 2: Background â€• 2.7 Non-Linearities

2.8

06-05-2024

Rapidly Exploring Random Trees

Rapidly-exploring Random Tree (RRT) is a sampling-based path planning algorithm, introduced by Steven M. LaValle in 1998[13]. The algorithm incrementally builds a tree of nodes,
each node a speciï¬c step length, ğ‘ , from the last. The tree is built by randomly sampling a
point in the conï¬guration space, and then extending the tree towards that point with ğ‘ . See
the entire algorithm in Algorithm 1.[13,14] Example 5 goes through an contextual example of
the algorithm.

Algorithm 1: The RRT Algorithm
Input: ğ‘¥start , ğ‘¥goal , ğ‘ , ğ‘ , ğ‘”tolerance
ğ‘‰ â† {ğ‘¥start }
ğ¸â†âˆ…
for ğ‘– = 1, â€¦, ğ‘ do
ğ‘¥random â† SampleRandomPoint()
ğ‘¥nearest â† NearestNeighbor(ğº = (ğ‘‰ , ğ¸), ğ‘¥random )
ğ‘¥new â† Steer(ğ‘ , ğ‘¥nearest , ğ‘¥random )
if CollisionFree(ğ‘¥nearest , ğ‘¥new ) then
ğ‘‰ â† ğ‘‰ âˆª ğ‘¥new
ğ¸ â† ğ¸ âˆª {(ğ‘¥nearest , ğ‘¥new )}
else
continue
end
if WithinGoalTolerance(ğ‘”tolerance , ğ‘¥new , ğ‘¥goal )
âˆ§ CollisionFree(ğ‘¥new , ğ‘¥goal ) then
ğ‘‰ â† ğ‘‰ âˆª ğ‘¥goal
ğ¸ â† ğ¸ âˆª {(ğ‘¥new , ğ‘¥goal )}
break
end
end
Output: ğº = (ğ‘‰ , ğ¸)

18

06-05-2024

2.8.1

Chapter 2: Background â€• 2.8 Rapidly Exploring Random Trees

RRT Functions

This section provides a mathematical description of the functions used in the RRT algorithm; functions SampleRandomPoint , NearestNeighbor , Steer , CollisionFree , and
WithinGoalTolerance . As in Algorithm 1, the RRT tree consists of vertices, ğ‘‰ , and edges,
ğ¸; together composing a graph, ğº = (ğ‘‰ , ğ¸). These denotations are used in the following descriptions.[15]

2.8.1.1 SampleRandomPoint() -> x
This functions takes no arguments, and returns a random point, ğ‘¥, in the conï¬guration space.
Most commonly this is done by drawing from a uniform distribution. Say that ğœ” is an element
in the set of all possible states in the conï¬guration space Î©, where âˆ€ğœ” âˆˆ Î© equation (36) holds.
[15]
SampleRandomPoint : ğœ” â†¦ {SampleRandomPointğ‘– (ğœ”)}

ğ‘–âˆˆâ„•0

âŠ‚ğ’³

(36)

That is; the set of all randomly sampled points, ğ’³rand , which is the result of the above mapping, is a subset of the conï¬guration space, ğ’³.

2.8.1.2 NearestNeighbor(G, x) -> v
Finds the nearest node ğ‘£ âˆˆ ğ‘‰ âŠ‚ ğ’³ in the tree to a given point. Takes in the graph, ğº = (ğ‘‰ , ğ¸),
and a point, ğ‘¥ âˆˆ ğ’³, see (37). This notion could be further speciï¬ed with a distance metric,
such as the Euclidean distance, as seen in (38), which return the node ğ‘£ âˆˆ ğ‘‰ that minimizes
the distance, â€–ğ‘¥ âˆ’ ğ‘£â€–, between the new point ğ‘¥ and an existing node ğ‘£.[15]
NearestNeighbor : (ğº, ğ‘¥) â†¦ ğ‘£ âˆˆ ğ‘‰

(37)

NearestNeighbor(ğº = (ğ‘‰ , ğ¸), ğ‘¥) = argminğ‘£âˆˆğ‘‰ â€–ğ‘¥ âˆ’ ğ‘£â€–

(38)

2.8.1.3 Steer(x, y, s) -> v
Creates a new node at a speciï¬c distance from the nearest node towards a given point. Takes
in two points ğ‘¥, ğ‘¦ âˆˆ ğ’³, and a step length ğ‘  âˆˆ â„+ ,. The new node ğ‘£ is created by moving ğ‘ 
distance from ğ‘¥ towards ğ‘¦. This way equation (39) returns a point ğ‘£ âˆˆ ğ’³ such that ğ‘£ is closer
to ğ‘¦ than ğ‘¥, which will either be ğ‘  closer, or if the randomly sampled point ğ‘¦ is within ğ‘  distance from ğ‘¥ to begin with, ğ‘£ will be at ğ‘¦. As such the inequality â€–ğ‘§ âˆ’ ğ‘¦â€– â‰¥ ğ‘  holds.[15]
Steer : (ğ‘¥, ğ‘¦, ğ‘ ) â†¦ ğ‘£ âˆˆ ğ’³

(39)

2.8.1.4 CollisionFree(x, y) -> p
Checks if the path between two nodes is collision-free. Takes in two points ğ‘¥, ğ‘¦ âˆˆ ğ’³, and
returns a boolean, ğ‘ âˆˆ {âŠ¤, âŠ¥}. The returned values, ğ‘, says something about whether the addition of node ğ‘¦ âˆˆ ğ’³ into the RRT tree is valid, given a proposed edge to the node ğ‘¥ âˆˆ ğ‘‰ .
Typically the validity notion depends on whether the path from ğ‘¥ to ğ‘¦ is collision-free, hence
the functionâ€™s name, but could include any other arbitrary constraints.[15]
19

Chapter 2: Background â€• 2.8 Rapidly Exploring Random Trees

06-05-2024

2.8.1.5 WithinGoalTolerance(t, x, g) -> p
Checks if a node is within the goal tolerance distance from the goal. As such the the functions
takes in the distance tolerance ğ‘¡, a node ğ‘¥ âˆˆ ğ‘‰ , and the goal state ğ‘” âˆˆ ğ’³. The function returns
a boolean, ğ‘ âˆˆ {âŠ¤, âŠ¥}, that tells us whether ğ‘¥ is within a euclidean distance ğ‘¡ from the goal
state ğ‘”. See (40) for the mathematical representation.[15]
WithinGoalTolerance : (ğ‘¡, ğ‘£, ğ‘”) â†¦ ğ‘ âˆˆ {âŠ¤, âŠ¥}

(40)

Example 5: Contextual RRT Application
Scenario: Letâ€™s look at an example, where the possible state space is two-dimensional euclidean space. A Robot wants to go from 2D position ğ‘¥ğ´ to ğ‘¥ğµ
Input: In Algorithm 1, the input is outlined to be a starting position, ğ‘¥start , a goal position
ğ‘¥goal , a step length ğ‘ , a maximum number of iterations, ğ‘ , and lastly, a goal tolerance,
ğ‘”tolerance .
Output: At the end of algorithm execution, the resulting graph is outputted as the combination of; ğ‘‰ , the set of vertices, and ğ¸, the set of edges.
Execution:
1. ğ‘‰ is initialised to contain the initial position of the robot ğ‘¥start = ğ‘¥ğ´ , thus the set {ğ‘¥ğ´ }.
ğ¸ is initialised to be empty.
2. Enter a for loop, that will maximally run ğ‘ times, but will break early if the goal is
reached.
Each iteration:
2.1. A random point, ğ‘¥random , is sampled from the conï¬guration space, by calling the
sampling function SampleRandomPoint().
2.2. The nearest existing node in the tree, ğ‘¥nearest , is found by NearestNeighbor(ğº =
(ğ‘‰ , ğ¸), ğ‘¥random ).
2.3. Thereafter, a new node, ğ‘¥new , is created by making a new node ğ‘  distance from
ğ‘¥nearest towards ğ‘¥random in the call to Steer(ğ‘ , ğ‘¥nearest , ğ‘¥random ).
Checks:
2.1. Only if the path from ğ‘¥nearest to ğ‘¥new is collision-free, the new node is added to the
tree. Otherwise, continue to the next iteration.
2.2. If the node is added to the tree, and it is within ğ‘”tolerance distance from ğ‘¥goal , and
the path from ğ‘¥new to ğ‘¥goal is collision-free, the goal is added to the tree, and the
loop is broken.

20

06-05-2024

2.9

Chapter 2: Background â€• 2.8 Rapidly Exploring Random Trees

Optimal Rapidly Exploring Random Trees

Optimal Rapidly-exploring Random Tree (RRT*) is an extension of the RRT algorithm, which
was introduced in 2011 by Sertac Karaman and Emilio Frazzoli in their paper Sampling-based
Algorithms for Optimal Motion Planning[14]. With only a couple of modiï¬cations to RRT, the
algorithm is able to reach asymptotic optimality, where the original algorithm makes no such
promises. The modiï¬cations are explained below:
M-1: Cost Function: The ï¬rst modiï¬cation is the introduction of a cost function, ğ‘(ğ‘£), for
each node, ğ‘£ âˆˆ ğ‘‰ . The cost function outputs the length of the shortest path from the
start node to the node ğ‘£. This modiï¬cation encodes an optimisable metric for each
branch, which enables the next modiï¬cation, M-2 , to take place.
M-2: Rewiring: The second modiï¬cation is the introduction of a neighbourhood radius, ğ‘Ÿ âˆˆ
â„+ , around each newly created node, which is used to search for nodes that can be
reached with a lower cost.
As such every time a new node is created, there is a possibility that other nodes
within that radius, will have a lower cost if they were to be connected to the new node.
Thus, comparing the nodesâ€™ old cost, and the cost they would have in case we connect
them to the newly created node, determines whether to rewire or not.

Algorithm 2: The RRT* Algorithm
Input: ğ‘¥start , ğ‘¥goal , ğ‘ , ğ‘ , ğ‘”tolerance
ğ‘‰ = {ğ‘¥start }
ğ¸=âˆ…
for ğ‘– = 1, â€¦, ğ‘› do
ğ‘¥rand â† Sample()
ğ‘¥nearest â† Nearest(ğ‘‰ , ğ¸, ğ‘¥rand )
ğ‘¥new â† Steer(ğ‘¥nearest , ğ‘¥rand )
if ObstacleFree(ğ‘¥nearest , ğ‘¥new ) then
ğ‘‰near â† Neighbourhood(ğ‘‰ , ğ¸, ğ‘¥new , ğ‘Ÿ)
ğ‘‰ â† ğ‘‰ âˆª {ğ‘¥new }
ğ‘nearest â† cost(ğ‘¥nearest ) + ğ‘(Line(ğ‘¥nearest , ğ‘¥new ))
ğ‘¥min â† MinCostConnection(ğ‘‰near , ğ‘¥new , ğ‘¥nearest , ğ‘nearest )
ğ¸ â† ğ¸ âˆª {[ğ‘¥min , ğ‘¥new ]}
Rewire(ğ‘‰near , ğ‘¥new )
end
if WithinGoalTolerance(ğ‘”tolerance , ğ‘¥new , ğ‘¥goal )
âˆ§ CollisionFree(ğ‘¥new , ğ‘¥goal ) then

21

Chapter 2: Background â€• 2.9 Optimal Rapidly Exploring Random Trees

06-05-2024

ğ‘‰ â† ğ‘‰ âˆª {ğ‘¥goal }
ğ¸ â† ğ¸ âˆª {[ğ‘¥new , ğ‘¥goal ]}
break
end
end
Output: ğº = (ğ‘‰ , ğ¸)

With the modiï¬cations made, the RRT* algorithm is shown in Algorithm 2[15]. Two important
blocks of the algorithm has been sectioned out in sub-algorithms Algorithm 3 and 4, which
are described along side the other new functions of RRT* under Section 2.9.1. The main parts
of the algorithm are visualised in Figure 6 as three steps:
Step 1: A new point has been sampled, deemed collision-free, and thus node ğ‘£new can be
added to the tree. But ï¬rst, we need to ï¬nd which existing node to connect to. Here,
ğ‘£nearest is chosen by the MinCostConnection algorithm, as it is the node that minimizes the total cost from the root to ğ‘£new , within the step-length radius ğ‘ .
Step 2: In preparation, rewiring candidates will be found, by looking at all nodes in the
tree, that are withing a certain reqiring radius, ğ‘Ÿ, from ğ‘£new . This is done by the
Neighbourhood function, which returns the set ğ‘‰near = {ğ‘›1 , ğ‘›2 , â€¦, ğ‘›ğ‘› }.
Step 3: This step is where the rewiring takes place. By looking at the nodes in ğ‘‰near , we can
compute each nodeâ€™s cost, ğ‘new with equation (41)
Cost(ğ‘£new ) + ğ‘(Line(ğ‘›ğ‘– , ğ‘£new ))

(41)

as if ğ‘£new were its parent. Denote the costs ğ¶near = {ğ‘1 , ğ‘2 , â€¦, ğ‘ğ‘› }. Now for each
node ğ‘›ğ‘– âˆˆ ğ‘‰near , check if ğ‘ğ‘– < ğ‘new , and if so, rewire the connection to make ğ‘£new the
parent of ğ‘›ğ‘– .

Figure 6. The RRT* algorithm drawn out in 3 steps. Firstly, a new node is sampled and added to the tree, where the
cost is lowest, looking in a radius of ğ‘  . Then nodes within a neighbourhood ğ‘Ÿ , are then rewired if their cost would
be lower by doing so.

2.9.1

RRT* Functions

Here the functions used in the RRT* algorithm are described in Algorithm 2. Functions from
base-RRT are not repeated here, as no change is made to them. The new functions are;
MinCostConnection , Rewire , Cost , Neighbourhood , Parent , and Line .

22

06-05-2024

Chapter 2: Background â€• 2.9 Optimal Rapidly Exploring Random Trees

2.9.1.1 MinCostConnection(V_near, v_new, v_init, c_init) -> v
This function is a main part of the RRT* modiï¬cation, as it attached the new node ğ‘£new , not
to the node nearest to the randomly sampled point in ğ’³, but to the node that minimizes the
cost from the root to ğ‘£new . This happens by looking at all nodes in a neighbourhood ğ‘‰near of
radius ğ‘Ÿ from ğ‘£new , and then ï¬nding the node that minimizes the cost. To begin with the initial
node ğ‘£init and its cost ğ‘init is passed to the function as the initial comparison point. The initial
comparison point is typically the nearest node in the tree, that would have been the parent
for ğ‘£new in RRT. The functionâ€™s operation is described in Algorithm 3.

Algorithm 3: Finding the Minimum Cost Connection
Input: ğ‘‰near , ğ‘¥new , ğ‘¥nearest , ğ‘nearest
ğ‘¥min â† ğ‘¥nearest
ğ‘min â† ğ‘nearest
for ğ‘¥near âˆˆ ğ‘‹near do
ğ‘near â† Cost(ğ‘¥near ) + ğ‘(Line(ğ‘¥near , ğ‘¥new ))
if CollisionFree(ğ‘¥near , ğ‘¥new ) âˆ§ ğ‘near < ğ‘min then
ğ‘¥min â† ğ‘¥near
ğ‘min â† Cost(ğ‘¥near ) + ğ‘(Line(ğ‘¥near , ğ‘¥new ))
end
end
Ouput: ğ‘¥min

2.9.1.2 Rewire(V_near, v_new)
The rewiring function is the second part of the RRT* optimisation steps, which changes previously established connections in the tree. The function uses the neighbourhood ğ‘‰near of nodes
in radius ğ‘Ÿ around ğ‘£new . For each ğ‘›ğ‘– âˆˆ ğ‘‰near , if the cost of ğ‘›ğ‘– with ğ‘£new as parent is lower
than the previously established cost for ğ‘›ğ‘– , the tree is rewired. The function is described in
Algorithm 4.

Algorithm 4: Rewiring
Input: ğ‘‰near , ğ‘¥new
for ğ‘¥near âˆˆ ğ‘‰near do
ğ‘near â† Cost(ğ‘¥new ) + ğ‘(Line(ğ‘¥new , ğ‘¥near ))
if CollisionFree(ğ‘¥new , ğ‘¥near ) âˆ§ ğ‘near < Cost(ğ‘¥near ) then
ğ‘¥parent â† Parent(ğ‘¥near )
ğ¸ â† ğ¸ \ {[ğ‘¥parent , ğ‘¥near ]}
ğ¸ â† ğ¸ âˆª {[ğ‘¥new , ğ‘¥near ]}
end

23

Chapter 2: Background â€• 2.9 Optimal Rapidly Exploring Random Trees

06-05-2024

end
Output: None

2.9.1.3 Cost(v) -> c
This function is used in Algorithm 2, 3, and 4 to access the cost ğ‘ of a node ğ‘£ âˆˆ ğ‘‰ . Typically
the cost is a distance, and as such; the sum of the Euclidean distances between all nodes if one
were to walk all the way back to the root node from ğ‘£. Thus a mapping from a node ğ‘£ âˆˆ ğ‘‰ to
a cost ğ‘ âˆˆ â„+ as shown in (42).
Cost : ğ‘£ â†¦ ğ‘ âˆˆ â„+

(42)

2.9.1.4 Neighbourhood(V, E, x, r) -> V_near
A more complex function, which returns a the set of all nodes in ğ‘‰ that are within a radius ğ‘Ÿ
from a potential new node ğ‘¥ âˆˆ ğ’³. If the conï¬guration space is ğ’³ = â„2 , then the neighbourhood ğ‘‰near is a subset of ğ‘‰ such that âˆ€ğ‘£ âˆˆ ğ‘‰near , â€–ğ‘£ âˆ’ ğ‘¥â€– â‰¤ ğ‘Ÿ. This mapping is described in
(43).
Neighbourhood : (ğ‘‰ , ğ¸, ğ‘¥, ğ‘Ÿ) â†¦ ğ‘‰near âŠ‚ ğ‘‰

(43)

2.9.1.5 Parent(v) -> p
Semantically denotes access to the parent node ğ‘ âˆˆ ğ‘‰ of a node ğ‘£ âˆˆ ğ‘‰ , see (44).
Parent : ğ‘£ â†¦ ğ‘ âˆˆ ğ‘‰

(44)

2.9.1.6 Line(x, y) -> l
Denotes the idea of the ï¬nding the line segment that is between two nodes ğ‘¥, ğ‘¦ âˆˆ ğ’³. This line
segment expresses the relationship between ğ‘¥ and ğ‘¦ in the conï¬guration space ğ’³. It can be
used, as seen in algorithms 2, 3 and 4, to calculate the cost that this line segment provides. This
is done by the function ğ‘(Line(ğ‘¥, ğ‘¦)), which, in case of a Euclidean conï¬guration space, and
thus cost would express a mapping from two points ğ‘¥, ğ‘¦ âˆˆ ğ’³ to a distance ğ‘™ âˆˆ â„+ , see (45).
Line : (ğ‘¥, ğ‘¦) â†¦ ğ‘™ âˆˆ â„+

ğ‘(Line(ğ‘¥, ğ‘¦)) = â€–ğ‘¥ âˆ’ ğ‘¦â€–

24

(45)

3

Methodology

3.1

Study 1: Reproduction

3.1.1

Implementation Language

This subsection describes the choice of implementation programming language used in both
the simulation and the reimplementation of the gbpplanner paper. Motivations for why the
language was chosen is laid out and argued for.
Both the simulation and the gbplanner reimplementation are written in the Rust programming language. Rust is a modern systems programming language

3.1.1.1 Why did we choose Rust?
â—† gbpplanner is implemented in C++. We are both familiar with C++, but have in previous
projects spent a lot
of time ï¬ghting its idiosyncrasies
Kristoï¬€er: What other alternatives were available? C++ Zig Julia Python, too slow, and lack
of a strong (enforced) type system had us worried about managing the implementation as the
project codebase would grow.
Kristoï¬€er: Explain some of the unique beneï¬ts of Rust.
â—† borrow checker
â—† ownership
â—† memory safety
â—† concurrency
â—† Performance
â—† Rich type system
â—† Rich library ecosystem
â—† Helpful error message
â—† Error handling, errors as values, no exceptions
â—† Exceptional tooling,

25

Chapter 3: Methodology â€• 3.1 Study 1: Reproduction

06-05-2024

Kristoï¬€er: Explain some of the drawbacks of Rust.
â—† Complexity, can be hard to learn
â—† Some design pattern/implementations are not trivial to implement. Especially self referential/recursive structures like graphs.
â—† Slow compilation
â—† Not as many very established libraries, like C++ which has been around for a lot longer

3.1.2

Signed Distance Field

3.1.3

Architecture

This section presents the architectural patterns used in the design of the simulation. First by
presenting the major architectural paradigm used, the ECS paradigm. What it is, how it works
and why it was chosen. Second how it results in changes compared to the original implementation of the gbplanner algorithm.

3.1.3.1 Entity Component System

Kristoffer
explain
what
is
meant by
data oriented programing

Entity Component System (ECS) is an architectural software design pattern speciï¬cally designed for data oriented programming . At the heart of it are three complementary concepts,
from which its name comes from: entities, components and systems:
Entity A collection of components with a unique id. Every object in the ECS world is an
entity. Most often the id a single unsigned integer.
Component Data scoped to a single piece of functionality. For example position, velocity,
rigid body, a timer etc.
System Functions that operate on the data by querying the ECS world for entities and components and updating them.
It if diï¬€erent from traditional Object Oriented Programming (OOP) based ways of modelling
At ï¬rst glance this representation/organisazation
that is designed to fully utilize modern computer hardware
memory hierarchies
Cache Locality
Kristoï¬€er: point out how it is diï¬€erent compared to traditional game engines/simulators
like Unity, Unreal, autodesk Isaac Sim
Kristoï¬€er: create ï¬gure explaining why ECS is cache friendly
Kristoï¬€er: Make a remark about the similarities with relational databases and query synteax like SQL. Also point out how the data is stored diï¬€erently, to handle concern about cache
friendlyness
data oriented design vs object oriented design
26

06-05-2024

Chapter 3: Methodology â€• 3.1 Study 1: Reproduction

The ECS architecture is not limited to game engines and simulations.
is versatile
Kristoï¬€er: what are its drawbacks?
Kristoï¬€er: cite ecs papers
Kristoï¬€er: create ï¬gure explaining ECS data store
entity similar to a primary key in a relational database
Kristoï¬€er: clarify that the data representation used by bevy is not one to one of the table
example
Entity (ID)

Transform

ğ‘

âœ”

ğ‘+1

âœ”

âœ”

âœ” [0.2, 0.8]

ğ‘+2

âœ”

âœ”

âœ” [âˆ’0.5, 0.0]

ğ‘+3

âœ”

âœ”

ğ‘+4

âœ”

âœ”

Robot

Camera

Velocity2d

âœ”

âœ” [0.0, 1.0]

Obstacle

â€¦

â€¦
ğ‘+ğ‘›

âœ”

âœ”

âœ” [1.0, 0.0]

Table 1. Structural layout of an ECS data store. Conceptually it is analogous to a table in relational database. âœ” in a
component column denotes that the entity has an instance of that component type e.g. entity ğ‘ + 2 has components:
{ Transform , Robot , Velocity2d }
fn move_robots(mut query: Query<(&mut Transform, &Velocity2d), With<Robot>>) {
for (mut transform, velocity) in &mut query {
...
}
}
Entity (ID)

Transform

ğ‘

âœ”

ğ‘+1

âœ”

âœ”

âœ” [0.2, 0.8]

ğ‘+2

âœ”

âœ”

âœ” [âˆ’0.5, 0.0]

ğ‘+3

âœ”

âœ”

ğ‘+4

âœ”

âœ”

Robot

Camera

Velocity2d

âœ”

âœ” [0.0, 1.0]

â€¦
ğ‘+ğ‘›

âœ”

âœ”

âœ” [1.0, 0.0]

The With<Robot> is a
where

27

Obstacle

â€¦

Chapter 3: Methodology â€• 3.1 Study 1: Reproduction

3.1.4

Kristoffer ï¬nd
citation
for
this
statement

06-05-2024

Graph Representation

Kristoï¬€er: talk about how our graph representation is diï¬€erent from theirs. Ours is more
faithful to how robots would represent the other robots in the environment compared to
theirs, since they use bidirectional std::shared_ptr , which is not useable in a scenario
where the algorithm run on diï¬€erent computer hosts, or just computer processes on the same
host.
There are several diï¬€erent ways of representing graph structures in computer memory.
Each with its own advantages and disadvantages. As explained in Section 2.4, the factorgraph
structure is a bipartite graph with undirected edges. Such a graph structure enforces little to
no constraints on what kind of memory representation are possible to use . In the original
work by Patwardhan et al.[5] a cyclic reference/pointer structure is used. They represent the
graph with a C++ class called FactorGraph , which each robot instance inherit from. Variable
and factor nodes are stored in two separate vectors; factors_ and variables_ , as shown in
the top of Code Reference 1.
Kristoï¬€er: Add static link to github, and a line range
// defined in `inc/gbp/FactorGraph.h`
class FactorGraph {
public:
std::vector<std::shared_ptr<Factor>> factors_;
std::vector<std::shared_ptr<Variable>> variables_;
// ...
};
// defined in `inc/gbp/Factor.h`
class Factor {
public:
// Vector of pointers to the connected variables. Order of variables matters
std::vector<std::shared_ptr<Variable>> variables_{};
// ...
};
// defined in `inc/gbp/GBPCore.h`
class Key {
public:
int robot_id_;
int node_id_;
};
// defined in `inc/gbp/Variable.h`
class Variable {
public:
// Map of factors connected to the variable, accessed by their key
std::map<Key, std::shared_ptr<Factor>> factors_{};
// ...
}

Code Reference 1. FactorGraph class declaration in inc/gbp/FactorGraph.h Kristoï¬€er: write proper caption

Edges between variable and factors are not stored as separate index values, but are instead
implicitly stored by having each factor storing a std::shared_ptr<Variable> to every variable it is connected to. Complementary every variable stores a std::shared_ptr<Factor> to
every factor it is connected to. This kind of structure is advantageous in that it easy access
the neighbours of node, given only a handle to the node. For example to send messages to

28

06-05-2024

Kristoffer mention that
their representation does
not map
well/reï¬‚ect how
each robot would
represent
connections to
external
robots
when running on
diï¬€erent
hosts

Chapter 3: Methodology â€• 3.1 Study 1: Reproduction

the neighbours of a node by directly invoking methods on the receiving node directly. This
structural pattern however is diï¬ƒcult to implement and discouraged in Rust due to its unique
language feature for managing memory; the ownership model. This model is comprised of
three rules, that together ensures memory safety and prevents memory issues like use after
free and memory leaks[16].
1. Each value has exactly one owner
2. There can only be one owner at a time.
3. When the owner goes out of scope, the value is dropped1.
One limitation of this system is that data structures with interior bidirectional references like
the gbpplanners factor graph representation are diï¬ƒcult to express, since there is no conceptual single owner of the graph. If a factor and a variable that are connected, both share a
reference to the memory region of each other, then there is not a well deï¬ned concept of who
owns who. Diï¬ƒcult does not mean impossible, and there are ways to express these kinds of
data structures using a Rust speciï¬c design pattern called the Interior Mutability pattern[16].
We decided not to use this pattern and instead work within the intended modelling constructs
of the Rust language. In the reimplementation the graph data structure uniqly owns all the
variable and factor nodes. And nodes in the in the graph store no interior references to nodes
they are connected to. The petgraph library is used for the actual graph datastructure. petgraph
is a versatile and performant graph data structure library providing various generic graph
data structures with diï¬€erent performance characteristics[17]. To choose which graph representation to use the following requirements were considered. The requirements are ordered
by priority in descending order:
1. Dynamic insertion and deletion of nodes. Robots connect to each others factorgraph when
they both are within the communication radius of each other and both their communication mediums are active/reachable. Likewise they disconnect when they move out of each
others communication or the other one is unreachable. This connection is upheld by the
InterRobot factor, which gets added and removed frequently.
2. Fast node iteration and neighbour search: In order to ensure collision free paths at tolerable
speeds, the GBP algorithm will have to run many times a second. The faster each factorgraph can be traversed the better.
3. The index of a node is valid for the lifetime of the node. Without this a lot of checks have
to be added every time the factorgraph is indexed to get a reference to a node.
petgraph supports the following ï¬ve types of graph representation:
Name

Description

Space Com- Backing Ver- Dynamic
plexity
tex Structure

Stable
Indices

Hashable
vertices

Graph

Uses an Adjacency List to
store vertices.

ğ‘‚(|ğ¸| + |ğ‘‰ |)

Vec<N>

âœ”

x

x

StableGraph

Similar
to
Graph , but it

ğ‘‚(|ğ¸| + |ğ‘‰ |)

Vec<N>

âœ”

âœ”

x

1
In Rust the term â€œdroppedâ€ is the preferred term to communicate that a value is destructed and its memory
deallocated.

29

Chapter 3: Methodology â€• 3.1 Study 1: Reproduction

Name

Description

06-05-2024

Space Com- Backing Ver- Dynamic
plexity
tex Structure

Stable
Indices

Hashable
vertices

keeps indices
stable across
removals.
GraphMap

Uses an associative array, but instead of storing vertices sequentially it
uses generated
vertex identiï¬ers as keys
into a hash
table, where
the value is
a list of the
verticesâ€™ connected edges.

IndexMap<N,
âœ”
ğ‘‚(|ğ¸| +
Vec<(N,
CompactDirection):>
|ğ‘‰ |) âˆ—

x

âœ”

MatrixGraph

Uses an Adjacency Matrix
to store vertices.

ğ‘‚(|ğ‘‰ 2 |)

Vec<N>

âœ”

x

x

CSR

Uses a sparse
adjacency matrix to store
vertices, in the
Compressed
Sparse
Row
(CSR) format.

ğ‘‚(|ğ¸| + |ğ‘‰ |)

Vec<N>

âœ”

x

x

Table 2. Available Graph Representations in the petgraph library. |ğ¸| is the number of edges and |ğ‘‰ | is the number
of nodes. The â€œBacking Node Structureâ€ lists which underlying data structure is used to store the associated of each
vertex. Vec<N> 2 is a growable array where items are placed continuous in memory[18]. IndexMap<N> is a special
hash map structure that uses a hash table for key-value indices, and a growable array of key-value pairs. Allows for
very fast iteration over nodes since their memory are densely stored in memory[19]. The â€œDynamicâ€ column labels
if the data structure supports vertices/edges being removed after initialization. The â€œHashable verticesâ€ columns list
if the data structure requires that the vertex type must be hashable.

Kristoffer ehhâ€¦
maybe

All ï¬ve graph representations support dynamic insertion and removal of vertices and edges
after initialization of the graph. So all of them satisfy the ï¬rst requirement. Four out of the ï¬ve
graph representations uses a Vec<N> as its underlying container for vertex instances. Vec<N>
are guaranteed to be continuous in memory ensuring fast iteration due to cache locality. At the
same time the relative diï¬€erence in iteration speed of using a the GraphMap structure should
not really be noticeable, given that it uses an IndexMap<N> , which in turn uses a Vec<(N, E)>
for its underlying storage of vertices. But it adds the additional constraint that vertices needs
to be hashable, which is impractical given the lack of non-unique immutable ï¬elds of the Node
struct. So all data structures support the second requirement. Only the StableGraph data
structure guarantees stable indices across repeated removal and insertion. Leaving it as the
2

Part of Rustâ€™s standard library

node_index

ï¬eld, but
Optionâ‡?

30

06-05-2024

Chapter 3: Methodology â€• 3.1 Study 1: Reproduction

sole viable choice left that meets all three requirements. Expressed using the petgraph library
the chosen Graph type is deï¬ned as:
type IndexSize = u16; // 2^16 - 1 = 65535
pub type NodeIndex = petgraph::stable_graph::NodeIndex<IndexSize>;
pub type Graph = petgraph::stable_graph::StableGraph<Node, (), Undirected, IndexSize>;

Code Reference 2.

Kristoffer ï¬nd
this number

Kristoï¬€er: explain () type parameter for edge data
IndexSize is a type parameter for the upperbound of the number of nodes the graph can
hold. In our experimentsSection 4 no individual factorgraph ever held more more than ~, so
a bound of 216 âˆ’ 1 was suï¬ƒcient, and takes up less space than 232 âˆ’ 1.
In terms of space complexity all ï¬ve candidates are close to equivalent, with four of them
using ğ‘‚(|ğ‘‰ | + |ğ¸|) space, and the MatrixGraph using ğ‘‚(|ğ‘‰ |2 ). Lack of suï¬ƒcient/enough
memory were not deemed and issue for the simulation. To support this claim let:
ğµ(ğ‘‡ ) = stack allocation of T in bytes

(46)

The standard library function std::mem::size_of::<T>() is used to calculate ğµ(ğ‘‡ )[18].
Then the size of a VariableNode and a FactorNode is:
ğµ(Variable) = 392 bytes

(47)

ğµ(Factor) = 408 bytes

(48)

A Node is modelled as a tagged union of a VariableNode and FactorNode so the size of a
node is, the size of the largest of the two variants, plus 8 bytes to store the tag[16]:
ğµ(ğ‘£ğ‘– ) = max(ğµ(Variable), ğµ(Factor)) + 8 bytes

(49)

No data is associated with an edge so the size of an edge is:
ğµ(ğ‘’ğ‘– ) = 0 bytes

(50)

ğµ(FactorGraph) = 200 bytes

(51)

An empty FactorGraph takes up:

Kristoï¬€er: explain functions and meaning of variable names
ğ‘obstacle (#ğ‘‰ ) = #ğ‘‰ âˆ’ 2

(52)

ğ‘dynamic (#ğ‘‰ ) = #ğ‘‰ âˆ’ 1

(53)

ğ‘interrobot (#ğ‘‰ , #ğ¶) = #ğ‘‰ Ã— #ğ¶

(54)

ğ‘factors (#ğ‘‰ , #ğ¶) = ğ‘obstacle (#ğ‘‰ ) + ğ‘dynamic (#ğ‘‰ ) + ğ‘interrobot (#ğ‘‰ , #ğ¶)

(55)

With a simulation of #ğ‘… robots and #ğ‘‰ variables, with each robot having #ğ¶ connections,
then the size is:
ğµ(Simulation(ğ‘…, ğ‘‰ , ğ¶)) = ğ‘… Ã— (ğµ(FactorGraph) + (ğ‘‰ + ğ‘factors (ğ‘‰ , ğ¶)) Ã— ğµ(Node))
(56)
31

Chapter 3: Methodology â€• 3.1 Study 1: Reproduction

06-05-2024

With ğ‘… = 20, ğ‘‰ = 10, ğ¶ = 10 the size is:
ğµ(Simulation(20, 10, 10)) = 1060640 bytes = 1.011505126953125 MiB

(57)

1.011505126953125 MiB is not a lot of memory for a modern computer. This of course, only
accounts for the stack allocated memory of each structure. For heap allocated structures like
dynamically sized matrices this only accounts for the heap pointer to the data and the length
of the allocated buï¬€er, and not the size of the buï¬€er. With 4 Degrees of Freedom (DOF) a
conservative estimate can be made by generalizing the heap allocation size of each node to be
the largest heap allocation of the possible node variants. Let
ğ»(ğ‘‡ ) = heap allocation of T in bytes

(58)

ğ»inbox (ğ‘‡ , ğ¶) = heap allocation of T's inbox with C connections in bytes

(59)

ğ‘‰current
ğ‘‰horizon

ğ»
ğ»inbox (ğ¶)
480 192 Ã— (1 + ğ¶)
480 192 Ã— (1 + ğ¶)

ğ»total (ğ¶)
672 + 192ğ¶
672 + 192ğ¶

ğ‘‰in-between 480 192 Ã— (3 + ğ¶) 1056 + 192ğ¶
192
864
ğ¹dynamic 672
ğ¹interrobot 416

384

800

[20]
Too summarize memory consumption should not be a limiting factor of simulating the system.
Not too many nodes in the graph, so we did not spend time benchmarking the various
backing memory models.
Since the memory required for each graph is not very high, we can aï¬€ord to use more space
for additional indices arrays
In addition to storing the graph itself each factorgraph use additional memory to store arrays of node indices of each node kind, to speed iteration
Adjacency matrix Adjacency list
one requirement the chosen graph model would need to satisfy, is the ability to update the
graph structure over time,
supported fast repeated iteration
trade some speed for edge lookup time, for having indices not being invalidated
consider insertion/deletion of nodes and edges as inï¬‚uencers of the graph data structure. j
using dedicated indices arrays for factors and variables to speed up iteration for queries
only requiring access to the nodes or variables.
Jens:

3.1.5

Asynchronous Message Passing

32

06-05-2024

Chapter 3: Methodology â€• 3.1 Study 1: Reproduction

Kristoï¬€er: show screenshots side by side of diï¬€erent elements of the simulation from theirs
and ours, e.g. visualisation of the factorgraph, or how we added visualisation of each variables
gaussian uncertainty
use this to argue on a non-measurable level why our implementation has is similar to
theirs / has been reproduced
Kristoï¬€er: List out all the conï¬guration parameters both the algorithm exposes and the
sim. Which one is identical to gbpplanner, and what values are sensible to use as defaults

Figure

7.
Variable
timesteps,
Kristoï¬€er: explain ï¬gure, and review design, use same variable names as rest of document

Kristoï¬€er: Explain the marginalization step/ algorithm They do explain it at ALL in the
paper so, we need another citation for it Maybe make a ï¬gure, or some colorful equations to
explain the step/slices
Kristoï¬€er: Explain how factors are abstracted using the Factor trait. How our implementation uses Composition instead of inheritance in C++
â—† What pros/cons does this bring?
trait Factor {
/// Name of the factor. Useful for debugging purposes
fn name(&self) -> &'static str;
/// Number of neighbours this factor expects
fn neighbours(&self) -> usize;
/// Whether the factor is linear or non-linear
fn linear(&self) -> bool;
/// The delta for the jacobian calculation
fn jacobian_delta(&self) -> f64;
/// The jacobian of the factor
fn jacobian(&self, state: &FactorState, x: &Vector<f64>) -> Cow<'_,
Matrix<f64>>;
/// Measurement function
fn measure(&self, state: &FactorState, x: &Vector<f64>) -> Vector<f64>;
/// First order jacobian (provided method)
fn first_order_jacobian(&self, state: &FactorState, x: Vector<f64>) ->
Matrix<f64> { ... }
}

3.2

Study 2: Improvements

33

Chapter 3: Methodology â€• 3.2 Study 2: Improvements

3.2.1

06-05-2024

Iteration Schedules

Scheduling, order in which we call internal and external iteration
Kristoï¬€er: Talk about how the paper make a distuinguishon about that internal and external iteration can be varied in amount per algorithm step. But does not mention how they are
ordered relative to each other i.e. what schedule they use.

Internal 10
Internal 5
Interleave Evenly

1

2

3

4

5

6

7

8

9

10

7

8

9

10

7

8

9

10

7

8

9

10

7

8

9

10

Soon as Possible

1

2

3

4

5

6

Late as Possible

1

2

3

4

5

6

Half at the beginning, Half at the end

1

2

3

4

5

6

Centered

1

3.3

2

3

4

5

6

Study 3: Extension

34

06-05-2024

3.3.1

Chapter 3: Methodology â€• 3.3 Study 3: Extension

Global Planning

Global planning has been made as an extension to the original GBP Planner software developed by Todo: cite . The original algorithm works very well on a local level, and lacks a global
overview of how to get from A to B as seen in Figure 8.
An example of the RRT algorithm in action can be seen in Figure 8.
Jens
more
about the
collider,
and the
environment representation.

Figure 8. RRT algorithm and environment integration.

3.3.1.1 Path Adherence
Jens: Approach 1: Simply perform RRT*, and use the resulting points as waypoints.
Jens: Approach 2: Use the RRT* algorithm to generate a path, then track the robots along it
with pose factors (traction factors).

35

Chapter 3: Methodology

3.4

06-05-2024

Study 4: Tooling & Design

Kristoï¬€er: talk about design of diï¬€erent conï¬g format design, especially formation and
environment

show examples of them, and how they allow for ï¬‚exibly and declaratively deï¬ne new simulation scenarios
formations:
- repeat-every:
secs: 8
nanos: 0
delay:
secs: 2
nanos: 0
robots: 1
initial-position:
shape: !line-segment
- x: 0.45
y: 0.0
- x: 0.55
y: 0.0
placement-strategy: !random
attempts: 1000
waypoints:
- shape: !line-segment
- x: 0.45
y: 1.25
- x: 0.55
y: 1.25
projection-strategy: identity

36

4

Results

4.1

Metrics

To objectively compare our reimplementation with the original GBP Planner, we measure and
compare the same four metrics: distance travelled, makespan, smoothness, and inter robot
collisions[5]:
1. Distance travelled The cumulative distance covered by the robot until it reaches its destination. Eï¬€ective trajectories aim to minimize this measure.
2. Makespan The overall duration for all robots to achieve their objectives. A collaborative
system of numerous robots should strive to reduce this measure.
3. Smoothness Continuous smooth trajectories are required in most cases, in order to be
realisable for the dynamics model of the robot and other real world constraints. .
Kristoffer like
what,
torque,
friction?

Kristoffer higher

Smoothness is a geometric property of the path traversed.
Kristoï¬€er: use same citation as them
dimensionless metric
This metric aims to quantify the smoothness of the robotâ€™s trajectories.
Î”

ğ¿ğ·ğ½ = âˆ’ ln(

2
(ğ‘¡final âˆ’ ğ‘¡start )3 ğ‘¡final Â¨
|ğ‘£
(ğ‘¡)|
âˆ«
ğ‘‘ğ‘¡)
2
ğ‘£max
ğ‘¡

(60)

start

is better?

where ğ‘¡ âˆˆ [ğ‘¡start , ğ‘¡final ] is the time interval the metric is measured over. ğ‘£(ğ‘¡) is the velocity of
a robot at time ğ‘¡, and ğ‘£max is the maximum velocity along the trajectory.
4. Inter robot Collisions Number of collisions between robots. The physical size of each robot is represented by a bounding circle. A collision between two robots happen when their
circles intersects.
In addition to the metrics used by by Patwardhan et al.[5] we also consider the following
metrics:
5. Root Mean Squared Error (RMSE)

37

Chapter 4: Results

4.2

06-05-2024

Scenarios

The performance of the reimplementation is evaluated in four diï¬€erent scenarios; Circle,
Clear Circle, and Junction. These scenarios adhere to the original paperâ€™s[5] experiments.
S-1 Circle: This environment presents 6 small obstacles in the middle of the environment. 30
robots are placed in a circle, centered on the environment with radius ğ‘Ÿ = 50ğ‘š around
the obstacles. The robots are tasked with reaching the opposite side of the circle.
S-2 Clear Circle: This environment is similar to the Circle scenario, but without any obstacles. Again 30 robots are placed in a circle, centered with ğ‘Ÿ = 50ğ‘š, and are tasked with
reaching the opposite side of the circle.
S-3 Junction: This environment is much more constrained, only with two roads; a vertical
and horizontal one, centered in their cross-axis. Thus creating a simple crossroads the
very center of the environment.
Speciï¬c details and parameters for each environment are presented in the following sections
4.2.1, 4.2.2, and 4.2.3. Additionally, environment visualisations are provided in ï¬gures 9, 10,
and 11.

4.2.1

Kristoffer is this
t0 ?

Circle
Param

Value

Î”ğ‘‡

0.1ğ‘ 

ğ‘€ğ‘…

10

ğ‘€ğ¼

50

ğœğ‘‘

1ğ‘š

ğœğ‘

1 Ã— 10âˆ’15

ğœğ‘Ÿ

0.005

ğœğ‘œ

0.005

ğ¶radius

50ğ‘š

ğ‘Ÿğ‘…
Initial speed

randomly sampled from ğ’°(2, 3)ğ‘š

ğ‘ğ‘…

{5, 10, 15, 20, 25, 30, 35, 40, 45, 50}

15 ğ‘š/ğ‘ 

ğ‘€ğ¼ Internal GBP messages
ğ‘€ğ‘… External inter-robot GBP messages
ğ‘ğ‘… Number of robots
ğ‘Ÿğ‘… Robot radius
Figure 9. Circle scenario.

38

06-05-2024

4.2.2

Chapter 4: Results â€• 4.2 Scenarios

Clear Circle
Figure 10. Clear Circle scenario.

4.2.3

Junction
Figure 11. Junction scenario.

4.3

Study 1: Reproduction

4.4

Study 2: Improvements

4.5

Study 3: Extension

4.6

Study 4: Tooling & Design

39

5

Discussion
Kristoffer discuss what
would
need to
change
from our
simulation to a
system
working
with real
world robots
â—† Detection of
neighbors

Kristoï¬€er: discuss why their implementation has a higher throughput that ours ğŸ˜” due to
graph representation. They use OpenMP

5.1

Future Work

5.1.1

Verify simulation results in a Real World setup

5.1.2 Use ray casting instead of sampling an SDF image of the
environment
â—† Use diï¬€erent behavior when communication is very bad. Either change the factor graph
by adding
new nodes with diï¬€erent policies or a new algorithm all together e.g. the game theory paper
we read at the start.
â—† Extend to 3D world. e.g. can it work with UAVS/drone. What would have to change?
â—† The state space is more complex. As a result the matrices being sent around are larger,
and more computationally costly
â—† What factors would have to change or be updated?
â—† Have other already done something similar
â—† Have the factorgraph be able to change the number of variable nodes during the lifetime
of the graph. I.e. having it dependant on the desired velocity. If we want to accelerate
the robot and have it move faster, it might be better to have more variables/longer future
horizon, while if we do not move very fast, fewer variables could be suï¬ƒcient and less
computationally taxing in that case.

40

06-05-2024

Chapter 5: Discussion â€• 5.1 Future Work

â—† Establish a simple network protocol to negotiate and establish the lifecycle of the bidirected
connection between two robots.
â—† How do two robots connect?
â—† How do they communicate?
â—† How do they check the connection is still alive?
â—† How do they reconnect? Is it any diï¬€erent from just disconnecting and then reconnecting, or is it necessary to have some kind of stateful protocol? Should be avoided
â—† How do they disconnect?
Kristoï¬€er: Experiment with diï¬€erent graph representations, e.g. matrix, csr, map based etc.
and test if they match theoretical performance proï¬les derived in the graph representation
section

41

6

Conclusion
Todo: conclusion

42

References
1.

Wisse M, Chiang T-C. WP1 - Competence Centers and Technical Expertise Management.
(2020)

2.

Kiadi M, Villar J R, Tan Q. Synthesized A* Multi-robot Path Planning in an Indoor Smart
Lab Using Distributed Cloud Computing. In: 15th International Conference on Soft Computing Models in Industrial and Environmental Applications (SOCO 2020). Herrero Ã, Cambra C, Urda D, Sedano J, QuintiÃ¡n H, Corchado E (Eds.), Springer International Publishing,
Cham, 580â€“589 (2021)

3.

Liu X, Peters L, Alonso-Mora J. Learning to Play Trajectory Games Against Opponents
with Unknown Objectives [Internet]. (2023). Available from: http://arxiv.org/abs/2211.1
3779

4.

Patwardhan A, Murai R, Davison A J. Distributing Collaborative Multi-Robot Planning
With Gaussian Belief Propagation. IEEE Robotics and Automation Letters [Internet]. 8(2),
552â€“559 (2023). Available from: https://ieeexplore.ieee.org/document/9976221/

5.

Patwardhan A, Murai R, Davison A J. Distributing Collaborative Multi-Robot Planning
With Gaussian Belief Propagation. IEEE Robotics and Automation Letters. 8(2), 552â€“559
(2023)

6.

Ortiz J, Evans T, Davison A J. A visual introduction to Gaussian Belief Propagation. arXiv
preprint arXiv:2107.02308. (2021)

7.

Dellaert F, Kaess M. Factor Graphs for Robot Perception. Foundations and Trends in Robotics [Internet]. 6(1â€“2), 11â€“13 (2017). Available from: http://www.nowpublishers.com/ar
ticle/Details/ROB-043

8.

Loeliger H-A. An introduction to factor graphs. IEEE Signal Processing Magazine [Internet]. 21(1), 28â€“41 (2004). Available from: https://ieeexplore.ieee.org/document/1267047

9.

Alevizos P. Factor Graphs: Theory and Applications. (2012)

10. Lecun Y, Chopra S, Hadsell R. A tutorial on energy-based learning. , 34â€“37 (2006)
11. Murai R, Ortiz J, Saeedi S, Kelly P H, Davison A J. A robot web for distributed manydevice localisation. IEEE Transactions on Robotics. (2023)
12. Eustice R M, Singh H, Leonard J J. Exactly Sparse Delayed-State Filters for View-Based
SLAM. IEEE Transactions on Robotics [Internet]. 22(6), 1100â€“1114 (2006). Available from:
https://ieeexplore.ieee.org/document/4020357
13. LaValle S. Rapidly-exploring random trees : a new tool for path planning. The annual
research report [Internet]. (1998). Available from: https://www.semanticscholar.org/pape
r/Rapidly-exploring-random-trees-%3A-a-new-tool-for-LaValle/d967d9550f831a8b3f5cb0
0f8835a4c866da60ad

43

Chapter R: ferences

06-05-2024

14. Karaman S, Frazzoli E. Sampling-based Algorithms for Optimal Motion Planning [Internet]. (2011). Available from: http://arxiv.org/abs/1105.1186
15. RRT Star - ERC Handbook [Internet]. . Available from: https://erc-bpgc.github.io/handbo
ok/automation/PathPlanners/Sampling_Based_Algorithms/RRT_Star/
16. The Rust Project Developers. The Rust Programming Language [Internet]. . Available
from: https://doc.rust-lang.org/book/
17. Anton Kochkov, bluss, AgustÃ­n Borgna, Bilal Mahmoud. petgraph. (2024)
18. The Rust Project Developers. The Rust Standard Library. (2024)
19. Josh Stone, bluss. indexmap. (2024)
20. Wheatman B, Xu H. Packed Compressed Sparse Row: A Dynamic Graph Representation.
In: 2018 IEEE High Performance extreme Computing Conference (HPEC), 1â€“7 (2018)

44
