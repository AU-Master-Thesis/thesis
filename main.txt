Multi-agent Collaborative Path Planning
Computer Engineering Master Thesis
Kristoﬀer Plagborg Bak Sørensen and Jens Høigaard Jensen
Department of Electrical and Computer Engineering
Aarhus University
Aarhus, Denmark

{201908140, 201907928}@post.au.dk
06-05-2024

Bloop

Supervisor: Andriy Sarabakha
Co-supervisor: Jonas le-Fevre Sejersen
{andriy, jonas.le.fevre}@ece.au.dk

i

Preface
This master thesis is titled “Multi-agent Collaborative Path Planning” and is devised by Jens
Høigaard Jensen and Kristoﬀer Plagborg Bak Sørensen. Both authors are students at Aarhus
University, Department of Electrical and Computer Engineering, enrolled in the Computer
Engineering Master’s programme. Both authors have completed a Bachelor’s degree in Computer Engineering under the same conditions.
The thesis has been conducted in the period from 29-01-2024 to 04-06-2024, and supervised
by Assistant Professor Andriy Sarabakha and co-supervised by PhD Jonas le-Fevre Sejersen.
Enjoy reading,
Jens Høigaard Jensen & Kristoﬀer Plagborg Bak Sørensen

ii

Abstract
This thesis explores… Todo: add more here

iii

Nomenclature
◆ Interior Mutability
◆ Test<T> denotes a type Test which is generic over any type T
◆ Every capital letter in monospace is some type. A type in rust is either a struct, a tagged
enum or a union.

Acronym Index

iv

AABB

Axis Aligned Bounding Box

ADE

Absolute Displacement Error

AOS

Array of Structures

APE

Absolute Position Error

ATE

Absolute Trajectory Error

BP

Belief Propagation

CSR

Compressed Sparse Row

DOF

Degrees of Freedom

ECS

Entity Component System

FDE

Final Displacement Error

GBP

Gaussian Belief Propagation

KL

Kullback-Leibler

LDJ

Log Dimensionless Jerk

MAE

Mean Absolute Error

MAP

Maximum A Posteriori

MAPE

Mean Absolute Position Error

OOP

Object Oriented Programming

RMSE

Root Mean Squared Error

RRT

Rapidly-exploring Random Tree

RRT*

Optimal Rapidly-exploring Random Tree

SIMD

Single Instruction, Multiple Data

SISD

Single Instruction, Single Data

SOA

Structure of Arrays

UAV

Unmanned Aerial Vehicle

v

Contents
Preface

ii

Abstract

iii

Nomenclature

iv

1

2
2
2
3
4
4

Introduction
1.1
1.2
1.3
1.4
1.5

2

Background
2.1
2.2
2.3
2.4
2.5
2.6
2.7
2.8
2.9

3

7
7
7
8
9
11
13
16
18
21
25
25
33
34
36

Study 1: Reproduction
Study 2: Improvements
Study 3: Extension
Study 4: Tooling & Design

Results
4.1
4.2
4.3
4.4
4.5
4.6

5

Related Works
Gaussian Models
Probabilistic Inference
Factor Graphs
Belief Propagation
Gaussian Belief Propagation
Non-Linearities
Rapidly Exploring Random Trees
Optimal Rapidly Exploring Random Trees

Methodology
3.1
3.2
3.3
3.4

4

Motivation
Problem Deﬁnition
Research Hypothesis
Research Questions
Research Objectives

37
37
38
39
39
39
39

Metrics
Scenarios
Study 1: Reproduction
Study 2: Improvements
Study 3: Extension
Study 4: Tooling & Design

Discussion

40

vi

5.1

6

Future Work

40

Conclusion

42

References

43

words

8836

characters

54174

normal pages

22.57

goal pages

100

goal characters

240000

pp./person/day

1.33

days left
22.57%
(22.57
pages)

29
77.43% (77.43 pages)

54.59%
77.17% (98 days)

12 todo

13 Jens

22.83% (29 days)

28 Kristoffer
53 remarks

vii

Kristoﬀer: make a statement about how we release our software for others to use .e.g license
and terms
Kristoﬀer: check all libraries we use are in accordance with our terms

1

1

Introduction

1.1

Motivation

Todo: A lot of this is probably in the original contract.
Harnessing the power of automation has been a key driver for the long term development of
human manufacturing and quality of life in the modern world since the industrial revolution
Todo: cite . Along with an increase in automation comes more playroom Todo: wording for
autonomous systems of higher complexity. Many current-day systems, such as the agriculture
industry[1] are on a trajectory toward autonomous multi-agent robotics. This trend can also
be projected onto more mainstream everyday tasks, such as manual transportation. The adoption of autonomous vehicles is increasing, and the technology is becoming more and more
mature. As the fraction of autonomous vehicles increases, the possibilities for eﬃcient traﬃc
expand exponentially with the ability to coordinate and communicate between vehicles. For
such systems to reach their full potential, it is preferred to move at high speeds and ensure
deadlocks are avoided. This is especially relevant in scenarios where the room for movement
is limited, this could be an indoor environment or roads with speciﬁc boundaries. Overall,
putting eﬀort towards the development of the multi-agent ﬁeld will help enable many current
and future systems to do more in less time, and with fewer resources.
With this development, the possibility and thus also demand for multi-agent systems have
become much more prevalent. This is especially impacting the ﬁeld of robotics, where separate agents can work together to achieve a common goal, or even work together to achieve
separate goals as eﬃciently as possible.
In the context of multi-agent systems, path planning is a key component. It is a process of
optimization to ﬁnd the most eﬃcient path for an agent to reach its goal. This is especially important in the context of multi-agent systems, where multiple agents need to coordinate their
movements to avoid collisions and reach their goals as eﬃciently as possible. This requires a
high level of collaboration and communication between the agents, as well as a high level of
collision avoidance to ensure that the agents do not collide with each other or with obstacles.

1.2

Problem Deﬁnition

Todo: A lot of this is probably in the original contract.
2

06-05-2024

Chapter 1: Introduction ― 1.2 Problem Deﬁnition

Eﬃcient and safe approaches to multi-agent planning have been studied extensively, with
many diﬀerent approaches and assumptions about the environment and capabilities of the
robotic agents. However, none of the existing approaches attempt to be able to operate in
changing communication conditions, in a decentralized manner. Morteza[2], 2021, implements
a modiﬁed A* algorithm that can handle dynamic obstacles in the environment. They also
propose an architecture where the path planning optimization is done in a centralized manner on a cloud server. This reduces the number of messages sent between the agents but introduces a single point of failure. Xinjie[3], 2023, uses a game-theoretic approach to do path
planning. Here agents cannot communicate with each other and instead have to predict how
other agents will move based on the rules that govern the movement of the agents. Similar
to how humans navigate in vehicle traﬃc, where the driver’s own assumption/belief about
how other drivers will follow the traﬃc law, is used to predict how other vehicles will move.
Aalok[4], 2023, uses Gaussian Belief Propagation to do path planning for multiple agents. It is
a decentralized approach using a data structure called a factor graph to model uncertainties
about the position and velocity of other agents.
The objective of this project is to build on top of the work of [4], and extend it to handle cases with limited communication possibilities, challenging environments, and non-holonomic kinematic constraints. The environment will be a static indoor environment at a logistics facility where robots are integrated into a baggage sorting handling system. Instead of
extending the original paper’s source code, we will reimplement our version of the Gaussian
Belief Propagation Planner algorithm from scratch. The intention of the rewrite is not only to
try and improve performance but also to get a better understanding of the algorithm, to make
it easier to extend it. The implemented solution will be tested in a simulated environment, with
common scenarios found at logistic facilities, such as three-way junctions and intersections.
Todo: Motivations for why our project is interesting
◆ What makes multi robot systems, better/more interesting than single robot systems?
◆ What challenges does it bring with it?
Todo: formulate a strong and clear deﬁnition of what path planning is, and ensure it is consistent with the rest of the document
Kristoﬀer: Enumerate what makes the Gaussian Belief Propagation algorithm attractive.
◆ Peer to Peer based
◆ No need to synchronize the ordering of messages in time
◆ Modular. New factors can be added to the factor graph to encode diﬀerent constraints.

1.3

Research Hypothesis

This thesis poses the following 4 hypothesis:
H-1 Reproducing the results of the original GBP Planner in a new programming language
will improve the software’s scientiﬁc communication and its extensibility.
H-2 Improvements can be made to the original work without transforming the software.
3

Chapter 1: Introduction ― 1.3 Research Hypothesis

06-05-2024

H-3 Extending the original GBP Planner software with a global planning layer will extend
the actors’ capability to move in complex environments, without degradation to the reproduced local cooperative collision avoidance, while maintaining a competitive level
of performance.
Layout
How to
measure,
frame
time, fps

H-4 Extensive tooling will create a great environment for others to understand the software
and extend it further. Furthermore, such tooling will make it easier to reproduce and
engage with the developed solution software.
From this point on, anything pertaining to the context of Hypothesis H-1 will be referred
to as Study 1: Reproduction, Hypothesis H-2 will be Study 2: Improvements, Hypothesis
H-3 will be Study 3: Extension, and Hypothesis H-4 will be Study 4: Tooling & Design.

1.4

Research Questions

Study 1 — Questions for hypothesis H-1 :
RQ-1.1 Which programming language will be optimal for scientiﬁc communication and extensibility?
RQ-1.2 Is it possible to reproduce the results of the original GBP Planner in the chosen programming language?

Wordings too
harsh?

Study 2 — Questions for hypothesis H-2 :
RQ-2.1 Which immediate improvements are obivous from looking at the original work?
RQ-2.2 Are these improvements possible without transforming the software?
RQ-2.3 Can these improvements be measured, and if so, how?
Study 3 — Questions for hypothesis H-3 :
RQ-3.1 Will global planning improve the actors’ capability to move in complex environments?
RQ-3.2 Will global planning degrade the reproduced local cooperative collision avoidance?
RQ-3.3 Will the global planning maintain a competitive level of performance?
Study 4 — Questions for hypothesis H-4 :
RQ-4.1 What kind of tooling will be most beneﬁcial for the software?
RQ-4.2 How can tooling help with future reproducibility and engagement with the software? Todo: maybe chance these or rephrase
RQ-4.3 How can tooling help with understanding and extending the software?
Todo: maybe chance these or rephrase

1.5

Research Objectives

Study 1: Reproduction
Objectives for Research Question RQ-1.1 :

4

06-05-2024

Chapter 1: Introduction ― 1.5 Research Objectives

O-1.1.1 Evaluate possible programming languages on several metrics for scientiﬁc communication and extensibility. Todo: rephrase, or mention diﬀerent metrics.
Layout
Learn by
reproducing

Objectives for Research Question RQ-1.2 :
O-1.2.1 Reimplement the original GBP Planner in the chosen programming language.
O-1.2.2 Evaluate whether the reimplementation is faithful to the original GBP Planner by
comparing the four metrics: distance travelled, makespan, smoothness, and collision
count.

Study 2: Improvements
Objectives for Research Question RQ-2.1 :
O-2.1.1 Identify immediate improvements to the original work.
Objectives for Research Question RQ-2.2 :
O-2.2.1 Implement the identiﬁed improvements without transforming the software.
O-2.2.2 Evaluate whether the improvements are transformative by comparing the four metrics: distance travelled, makespan, smoothness, and collision count.
Objectives for Research Question RQ-2.3 :
O-2.3.1 Use the measurements from O-2.2.2 to evaluate the improvements.

Study 3: Extension
Objectives for Research Question RQ-3.1 :
O-3.1.1 Implement a global planning layer in the reimplemented GBP Planner.
O-3.1.2 Evaluate the actors’ capability to move in complex environments by looking at the
four metrics: distance travelled, makespan, smoothness, and collision count, comparing against the reimplemented reproduction and the original GBP Planner.
Objectives for Research Question RQ-3.2 :
O-3.2.1 Compare the four metrics: distance travelled, makespan, smoothness, and collision
count of the reimplemented reproduction with and without the global planning
layer.
Objectives for Research Question RQ-3.3 :
O-3.3.1 Compare performance metrics of the reimplemented reproduction with and without
the global planning layer against the original GBP Planner.

Study 4: Tooling & Design
Objectives for Research Question RQ-4.1 :
O-4.1.1 Analyse and evaluate diﬀerent kinds of tooling that can be beneﬁcial for the software.
O-4.1.2 Pick the most beneﬁcial tooling approach for the software.
Objectives for Research Question RQ-4.2 :
O-4.2.1 Implement tooling to help with future reproducibility and engagement with the software. Todo: maybe chance these or rephrase
Objectives for Research Question RQ-4.3 :

5

Chapter 1: Introduction ― 1.5 Research Objectives

06-05-2024

O-4.3.1 Implement tooling to help with understanding and extending the software.
Todo: maybe chance these or rephrase
Todo: Part of the argument for H-2: Furthermore, a language with that shares qualities
Kristoﬀer: wording with modelling languages will improve the software’s ability to communicate scientiﬁc results.
Todo: argument for rust: Half way a modelling language, which is optimal for scientiﬁc
communication and extensibility.
Jens: make ﬁgure that shows the connection of all these, including outlining which parts
are which study.

6

2

Background

Firstly, background knowledge on related works is presented, in 2.1, followed by a technical
introduction to the underlying theory for this thesis. In Section 2.2 and 2.3, Gaussian models
and probabilistic inference are introduced, respectively. These two topics are the theoretical
base for understanding factor graphs and the corresponding methods for inference and reasoning about them, as detailed in sections 2.4-2.6.

2.1

Related Works

Originally Murai et al. showed, with their A Robot Web for Distributed Many-Device Localisation, that Gaussian Belief Propagation (GBP) can be utilised to solve multi-agent localisation.
Then Patwardhan et al. showed that the same algorithm structure can be adapted to multiagent planning. This thesis aims to extend the work by Patwardhan et al.[5] to include a global
planning layer, which can provide a more robust solution to the multi-agent planning in highly
complex environments. As such [5] will be covered more thoroughly in its own dedicated
section below, see Section 2.1.1.
Todo: maybe the ‘dedicated section’ is simply the rest of the background which contains the
relevant theory.

2.1.1

GBP Planner

2.2

Gaussian Models

To eventually understand GBP, the underlying theory of Gaussian models is detailed in this
section. Gaussian models are often chosen when representing uncertainty due to the following reasons:
Reason 1 Realistic Modeling: Gaussian models eﬀectively capture the way many physical
phenomena and sensor readings are distributed in the real world.[6]
Reason 2 Mathematical Simplicity: Gaussian models have a clean mathematical structure, making them easy to work with.[6]
7

Chapter 2: Background ― 2.2 Gaussian Models

06-05-2024

Reason 3 Computational Efﬁciency: Calculations involving Gaussian models can be performed using straightforward formulas, keeping computations fast.[6]
Reason 4 Flexibility: Gaussian models maintain their form under common statistical operations (marginalization, conditioning, products), ensuring ease of manipulation
within robotic systems.[6]
A Gaussian distribution can be represented in the exponential energy form, which is a common way of representing probability distributions. The exponential energy form is deﬁned as
in (1)[6]:
𝑝(𝑥) =

1
exp(−𝐸(𝑥))
𝑍

(1)

Jens: What does 𝑍 and 𝐸 mean?
In the exponential energy form, a Gaussian model can be represented in two ways; the
moments form and the canonical form. The moments form is deﬁned by the mean vector, 𝜇,
and the covariance matrix, Σ.[6] The canonical form is deﬁned by the information vector, 𝜂,
and the precision matrix, Λ. The energy parameters, energy equations, and computational efﬁciency for certain aspects of the two forms are compared in Figure 1.
Moments Form

Canonical Form

Parameters:

Parameters:

Mean:

𝜇

Information:

𝜂 = Σ−1 𝜇

Covariance:

Σ

Precision:

Λ = Σ−1

Energy Equation:

Energy Equation:

𝐸(𝑥) = 12 (𝑥 − 𝜇)⊤ Σ−1 (𝑥 − 𝜇)

𝐸(𝑥) = 12 𝑥⊤ Λ𝑥 − 𝜂⊤ 𝑥

Computational Eﬃciency:

Computational Eﬃciency:

Marginalisation:

Cheap

Marginalisation:

Expensive

Conditioning:

Expensive

Conditioning:

Cheap

Product:

Expensive

Product:

Cheap

Figure 1. Camparison between the moments form and the canonical form of representing Gaussian models.[6]
Jonas
Is
it necessary to say
“Figure
from”

As outlined in Figure 1 the Canonical Form is much more computationally eﬃcient when it
comes to conditioning and taking products, while the Moments Form excels at marginalisation.
[6] Jens: explain why this is the case

2.3

Probabilistic Inference

To contextualise factor graph inference, the underlying probabilistic inference theory is introduced. The goal of probabilistic inference is to estimate the probability distribution of a
8

06-05-2024

Chapter 2: Background ― 2.3 Probabilistic Inference

set of unknown variables, 𝑋, given some observed or known quantities, 𝐷. This is done by
combining prior knowledge with 𝐷, to infer the most likely distribution of the variables.[6]
See Example 1.

Example 1: Probabilistic Inference in Meteorology
An everyday example of probabilistic inference is in the ﬁeld of meteorology. Meteorologists use prior knowledge of weather patterns (𝐷), combined with observed data to infer
the most likely weather forecast for the upcoming days (𝑋).
Baye’s rule is the foundation of probabilistic inference, and is used to update the probability
distribution of a set of variables, 𝑋, given some observed data, 𝐷. The rule is deﬁned as in
(2)[6]:
𝑝(𝑋|𝐷) =

𝑝(𝐷|𝑋)𝑝(𝑋)
𝑝(𝐷)

(2)

This posterior distribution describes our belief of 𝑋, after observing 𝐷, which can then be used
for decision making about possible future states.[6] Furthermore, when we have the posterior,
properties about 𝑋 can be computed;
Property 1 The most likely state of 𝑋, the Maximum A Posteriori (MAP) estimate 𝑋MAP , is
the state with the highest probability in the posterior distribution. See (3)[6]:
𝑋MAP = argmax𝑋 𝑝(𝑋|𝐷)

(3)

Property 2 The marginal posteriors, summarising our beliefs of individual variables in 𝑋,
can be computed by marginalising the posterior distribution, see (4)[6]:
𝑝(𝑋𝑖 |𝐷) = ∑ 𝑝(𝑋|𝐷)
𝑋\𝑥𝑖

2.4

(4)

Factor Graphs

A factor graph is a bipartite graph, where the nodes are divided into two disjoint sets; variables and factors. And exempliﬁcation of a factor graph and important intuition is shown in
Example 2. The edges between nodes each connect one from each set, and represent the dependencies between the variables and factors. A factor graph represents the factorisation of
any positive joint distribution , 𝑝(𝑋), as stated by the Hammersley-Cliﬀord Theorom. That is,
a product of factors for each clique of variables in the graph, which can be seen in (5)[5–9].
𝑝(𝑋) = ∏ 𝑓𝑖 (𝑋𝑖 )
{𝑖}

9

(5)

Chapter 2: Background ― 2.4 Factor Graphs

06-05-2024

Thus interpreting this, the factors are not necessarily in themselves probabilities, but rather
the functions that determine the probabilities of the variables.[8,9] Additionally, it can be useful to present factor graphs as energy-based models[10], where, as seen in (6)[6], each factor
𝑓𝑖 is associated with an energy 𝐸𝑖 > 0:[6]
𝑓𝑖(𝑋𝑖 ) = exp(−𝐸𝑖(𝑋𝑖 ) )

(6)

This presentation also gives another way of ﬁnding the MAP estimate, by ﬁnding the state
with the lowest energy in the factor graph, see (7)[6]:
𝑋MAP = arg min𝑋 − log 𝑝(𝑋)
= arg min𝑋 ∑ 𝐸𝑖(𝑋𝑖 )

(7)

𝑖

Example 2
An example factor graph is visualised in Figure 2, with variables {𝑣1 , …, 𝑣4 } and factors
{𝑓1 , …, 𝑓4 }. Writing out the visualised factor graph produces:
𝑝(𝑣1 , 𝑣2 , 𝑣3 , 𝑣4 ) =
Wordings

1
𝑓 (𝑣 , 𝑣 , 𝑣 )𝑓 (𝑣 , 𝑣 )𝑓 (𝑣 , 𝑣 )𝑓 (𝑣 )
𝑍 1 1 2 3 2 3 4 3 3 4 4 4

(8)

Should
this
be
here?
gbpvisual-introduction

doesn’t
have it
Figure 2. A factor graph is a bipartite graph, where the nodes are divided into two sets; variables and factors.
Variables are represented as red circles , and factors as blue squares . The edges between the nodes represent
the dependencies between the variables and factors.

The factor graph is a generalisation of constraint graphs, and can represent any join function.
Moreover, the factor graph structure enables eﬃcient computation of marginal distributions
through the sum-product algorithm.[8,9] The sum-product algorithm is detailed in Section 2.5.

10

06-05-2024

Chapter 2: Background ― 2.4 Factor Graphs

Figure 3. Here shown are two factor graphs, one for a green robot, and one for a purple robot. In this speciﬁc case
the two robots are close to each other, and perfectly aligned. At the top, the planning horizon is shown in red , 𝑛
times-steps into the future, {𝑡1 , 𝑡2 , …, 𝑡𝑛 }. Variables are visualised as circles, and factors as squares.

Wordings 𝑣
1

Wordings

In Figure 3 two joint factor graphs are visualised. The ﬁrst variables in each factor graph
𝑣1 , represent the location of a green and purple robot respectively. Each robot has a corresponding factorgraph, where the ﬁgure shows how the two factor graphs are connected with
interrobot factors 𝑓𝑖 when they are close enough to each other. Variables 𝑣2 , …, 𝑣𝑛 represent
the future predicted states of the robot respectively at timesteps 𝑡2 , …, 𝑡𝑛 , where 𝑡1 is the current time.

planned

2.5
Wordings

Belief Propagation

The process of performing inference on a factor graph is done by passing messages between
the variables and factors. Figure 4 visualises the two major steps; Variable Iteration and Factor
Iteration, each with two sub-steps; an internal update, and a message passing step.
Step 1 Variable update
Step 2 Variable to factor message

Step 3 Factor update
Step 4 Factor to variable message

Figure 4. The four steps of propagating messages in a factor graph. Firstly, the variables are internally updated, and
new messages are sent to neighbouring factors , who then update internally, sending the updated messaages back
to neighbouring variables .

11

Chapter 2: Background ― 2.5 Belief Propagation

06-05-2024

Note that this ﬁgure shows the Variable Iteration ﬁrst, however, performing the Factor Iteration ﬁrst is also a valid, the
main idea is simply that they are alternating.

Jens: context for BP and the sum-product algorithm
In Step 1 the computation of the marginal distribution of a variable 𝑥𝑖 takes place. This
is done by ﬁnding the product of all messages from neighbouring factors 𝑓𝑗 to 𝑥𝑖 , as seen in
(9)[5,6,11].
𝑚𝑥𝑖 = ∏ 𝑚𝑓𝑠 →𝑥𝑖

(9)

𝑠∈𝑁(𝑖)

Secondly, in Step 2 the variable to factor messages 𝑚𝑥𝑖 →𝑓𝑗 are computed as described in (10)
[5], which is a product of all messages from neighbouring factors 𝑓𝑠 except 𝑓𝑗 .[5,6,11]
𝑚𝑥𝑖 →𝑓𝑗 =

∏

𝑚𝑓𝑠 →𝑥𝑖

𝑠∈𝑁(𝑖)\𝑗

(10)

The factor to variable messages 𝑚𝑓𝑗 →𝑥𝑖 are described in (11)[5], where the message is the
product of the factor 𝑓𝑗 and the messages from all neighbouring variables 𝑥𝑖 except 𝑥𝑖 itself.
[5,6,11] This corresponds to the entire Factor Iteration, i.e. Step 3 and Step 4 , also shown
in Figure 4.
𝑚𝑓𝑗 →𝑥𝑖 = ∑ 𝑓𝑗 (𝑋𝑗 )
𝑋𝑗 \𝑥𝑖

∏
𝑘∈𝑁(𝑗)\𝑖

𝑚𝑥𝑘 →𝑓𝑗

(11)

Jens: ﬁnish this section
Originally Belief Propagation (BP), was created for inference in trees, where each message
passing iteration is synchronous. This is a simpler environment to guarantee convergence in,
and in fact after one synchronous message sweep from root to leaves, exact marginals would
be calculated. However, factor graphs, as explained earlier, are not necessarily trees; they can
contain cycles, and as such loopy BP is required. Loopy BP, instead of sweeping messages,
applies the message passing steps to each each at every iteration, but still in a synchronous
fashion.[6] Jens: more citation for loopy BP
The expansion to loopy graphs is not without its challenges, as the convergence of the
algorithm is not guaranteed. As such the problem transforms from an exact method to and
approximation. This means, that instead of minimising the factor energies through MAP directly, loopy BP minimises the Kullback-Leibler (KL) divergence between the true distribution
and the approximated distribution, which can then be used as a proxy for marginals after satisfactory optimisation.[6]
Loopy BP is derived via the Bethe free energy, which is a constrained minimisation of an
approximation of the KL divergence. As the Bethe free energy is non-convex, the algorithm
isn’t guaranteed to converge, and furthermore, it might converge to local minima in some
cases. It has been shown that empirically loppy BP is very capable of converging to the true
marginals, as long as the graphs aren’t highly cyclic.[6]
Wordings too
loopy?
Is loopy
and cyclic
the same
thing?

12

06-05-2024

2.6

Chapter 2: Background

Gaussian Belief Propagation

Jens: do this 😄
Having introduced both Gaussian models, and BP, we can now take a look at GBP. GBP is
a variant of BP, where, due to the closure properties Jens: cite of Gaussians, the messages
and beliefs are represented by Gaussian distributions. In its base form GBP works by passing
Gaussians around in the Canonical Form, i.e. the messages and beliefs contain the precision
matrix, Λ, and the information vector 𝜂. As mentioned earlier, general BP is not guaranteed to
compute exact marginals, however, for GBP exact marginal means are guaranteed, and even
though the variances often converge to the true marginals, there exists no such guaranteed.[6]
In a factor graph, where all factors are Gaussian, and since all energy terms are additive in
the Canonical Form, the energy of the factor graph is also Gaussian, which means that one
can represent it as a single multivariate Gaussian. See the equation for this joint distribution
in (12)[6]:
1
𝑝(𝑋) ∝ exp(− 𝑋 ⊤ Λ𝑋 + 𝜂⊤ 𝑋)
2

(12)

2.6.1.1 MAP Inference
In the context of GBP, the MAP estimate can be found by the parameters 𝑋MAP that maximises
the joint distribution in (12). The total energy can then be written as (13)[6]
∇𝑋 𝐸(𝑋) = ∇𝑋 log 𝑃 (𝑋) = −Λ𝑋 + 𝜂

(13)

which is the gradient of the log-probability, and can be set to zero, ∇𝑋 𝐸 = 0, to ﬁnd the MAP
estimate, which, in GBP is reduced to the mean 𝜇, as seen in (14)[6]:
𝑋MAP = Λ−1 𝜂 = 𝜇

(14)

2.6.1.2 Marginal Inference
The goal of marginal inference in GBP is to ﬁnd the per variable marginal posterior distributions. In the Moments Form this looks like (15)[6]:
𝑝(𝑥𝑖 ) = ∫ 𝑝(𝑋) 𝑑𝑋−𝑖
1
∝ exp(− (𝑥𝑖 − 𝜇𝑖 )⊤ Σ−1
𝑖𝑖 (𝑥𝑖 − 𝜇𝑖 ))
2

(15)

where 𝑋{−𝑖} is the set of all variables except 𝑥𝑖 , and Σ𝑖𝑖 is the 𝑖th diagonal element of the
covariance matrix Σ = Λ. The marginal posterior distribution is then a Gaussian with mean
𝜇𝑖 and variance Σ𝑖𝑖 . Furthermore, 𝜇𝑖 is the 𝑖th element of the joint mean vector 𝜇, as in (12).
With the understanding from 2.6.1.1 and 2.6.1.2 inference in GBP ends up being a matter of
solving the linear system of equations (16)[6]:
𝐴𝑥 = 𝑏 ⇒ Λ𝜇 = 𝜂
13

(16)

Chapter 2: Background ― 2.6 Gaussian Belief Propagation

06-05-2024

Where MAP inference solves for the mean, 𝜇, and marginal inference ﬁnds the covariance, Σ,
by solving for the block diagonal of Λ−1 , and indirectly also the mean, 𝜇.

2.6.2

Variable Update

The variable belief update happens by taken the product of incoming messages from nerighbouring nodes, here denoted as 𝑁 (𝑖), as seen in (17)[6]:
𝑏𝑖(𝑥𝑖 ) = ∏ 𝑚𝑓𝑠 →𝑥𝑖

(17)

𝑠∈𝑁(𝑖)

Writing out the Gaussian message on Canonical Form becomes (18)[6]:
1
𝑚 = 𝒩−1 (𝑥; 𝜇, Λ) ∝ exp(− 𝑥⊤ Λ𝑥 + 𝜂⊤ 𝑥)
2

(18)

Fortunately, as the messages are stored on Canonical Form, the product in (17) is the same as
summing up the information vectors and precision matrices, as seen in (19)[6]:
𝜂𝑏𝑖 = ∑ 𝜂𝑓𝑠 →𝑥𝑖

and

Λ𝑏𝑖 = ∑ Λ𝑓𝑠 →𝑥𝑖

𝑠∈𝑁(𝑖)

2.6.3

𝑠∈𝑁(𝑖)

(19)

Variable to Factor Message

The variable to factor message passing is described in (20)[6], where the message is the product of the incoming messages from all neighbouring factors 𝑓𝑗 except the factor 𝑓𝑖 itself, same
as described in (10)[5]:
𝑚𝑥𝑖 →𝑓𝑗 =

∏

𝑚𝑓𝑠 →𝑥𝑖

(20)

𝑠∈𝑁(𝑖)\𝑗

Again in this case, the message is sent in the Canonical Form form, and as such the outgoing
messages can simply be computed by summing up the information vectors and precision matrices, as seen in (21)[6]:
𝜂𝑥𝑖 →𝑓𝑗 =

∑ 𝜂𝑓𝑠 →𝑥𝑖

and

𝑠∈𝑁(𝑖)\𝑗

Λ𝑥𝑖 →𝑓𝑗 =

∑ Λ𝑓𝑠 →𝑥𝑖
𝑠∈𝑁(𝑖)\𝑗

14

(21)

06-05-2024

2.6.4

Chapter 2: Background ― 2.6 Gaussian Belief Propagation

Factor Update

Jens: describe how factor distance is marginalised and factors are updated
1. Update linearisation point
2. Measurement & jacobian around linearisation point The measurement residual is
𝑚𝑟 = 𝑚(𝑋0 ) − 𝑚(𝑋𝑛 )

(22)

Where 𝑋0 is the conﬁguration at 𝑡0 , and 𝑋𝑛 is the conﬁguration at the current timestep
𝑡𝑛 .
3. Factor potential update
Λ𝑝 = 𝐉⊤ Λ𝑀 𝐉
𝜂𝑝 = 𝐉⊤ Λ𝑀 (𝐉𝑙𝑝 + 𝑚𝑟 )

(23)

Where Λ𝑝 and 𝜂𝑝 denotes the precision matrix and information vector of the factor potential, and Λ𝑀 is the measurement precision matrix.
4. Factor marginalisation

2.6.5

Factor to Variable Message Passing

Before marginalising, messages from nerighbouring variables are aggregated into a single
message, as seen in (24)[6]:
𝑚𝑓𝑖 →𝑥𝑗 =

∏

𝑚𝑥𝑠 →𝑓𝑖

𝑠∈𝑁(𝑗)\𝑖

(24)

Example 3
Consider a factor 𝑓 connected to 3 variables; 𝑥1 , 𝑥2 , 𝑥3 , and we want to compute the message to be passed to variable 𝑥1 . Write the factor out as a Gaussian distribution, see (25)[6]:
𝑓([𝑥1 𝑥2 𝑥3 ]) = 𝒩−1 ([𝑥1 𝑥2 𝑥3 ]; 𝜂𝑓 , Λ𝑓 )

(25)

Here, the two Gaussian parameters 𝜂𝑓 and Λ𝑓 can be expanded to see the individual contributions from each variable, as seen in (26)[6]:
𝜂𝑓1
⎡𝜂 ⎤
𝜂𝑓 = ⎢ 𝑓2 ⎥
𝜂
⎣ 𝑓3 ⎦

and

Λ
Λ
Λ
⎡ 𝑓11 𝑓12 𝑓13 ⎤
Λ𝑓 = ⎢Λ𝑓21 Λ𝑓22 Λ𝑓23 ⎥
⎢
⎥
Λ𝑓31 Λ𝑓32 Λ𝑓33
⎣
⎦

15

(26)

Chapter 2: Background ― 2.6 Gaussian Belief Propagation

06-05-2024

As described earlier, ﬁrstly step is to compute the message to be passed to 𝑥1 , which is
the product of the incoming messages from 𝑥2 and 𝑥3 , as seen in (27), as we are on the
Canonical Form this is a summation, and yields the Gaussian, 𝒩(𝜂𝑓′ , Λ′𝑓 )[6]:
𝜂𝑓1
⎡
⎤
𝜂𝑓′ = ⎢𝜂𝑓2 + 𝜂𝑥2 →𝑓 ⎥
⎢
⎥
𝜂𝑓3 + 𝜂𝑥3 →𝑓
⎣
⎦

and

Λ
Λ𝑓12
Λ𝑓13
⎡ 𝑓11
⎤
⎥ (27)
Λ𝑓23
Λ′𝑓 = ⎢Λ𝑓21 Λ𝑓22 + Λ𝑥2 →𝑓
⎢
⎥
Λ𝑓31
Λ𝑓32
Λ𝑓33 + Λ𝑥3 →𝑓
⎣
⎦

Now as we are passing a message to 𝑥1 , we have to marginalise out all other variables,
𝑥2 and 𝑥3 . This is done by the marginalisation equations given by [12] for Gaussians in
Canonical Form. See (28) and (29) for the joint distribution over variables 𝑎 and 𝑏[6,12].
𝜂𝑎
𝜂 = [𝜂 ]

Λ𝑎𝑎 Λ𝑎𝑏
]
Λ𝑏𝑎 Λ𝑏𝑏

(28)

and Λ𝑎𝑎 = Λ𝑀𝑎 − Λ𝑎𝑏 Λ−1
𝑏𝑏 Λ𝑏𝑎

(29)

and

𝑏

𝜂𝑀𝑎 = 𝜂𝑎 + Λ𝑎𝑏 Λ−1
𝑏𝑏 𝜂𝑏

Λ=[

Now to marginalise, perform the two steps:
Step 1: Reorder the vector 𝜂𝑓′ and the matrix Λ′𝑓 to bring the contribution from
the recipient 𝑥1 to the top.
In our case no reordering is to be done, as 𝑥1 is already at the top.
Jens: maybe an example where reordering is necessary is better
Step 2: Recognise the subblocks 𝑎 and 𝑏 from (28) and (29).
In our case 𝑎 = 𝑥1 and 𝑏 = [𝑥2 𝑥3 ].

2.7

Non-Linearities

Jens: and this
Non-linear factors can exist, however, to understand the impact, let’s ﬁrst look at linear
factors. A factor is usually modeled with data 𝑑. Equation (30)[6] shows how this is written:
𝑑 ∼ 𝑚(𝑋𝑛 ) + 𝜖

(30)

Here, 𝑚(𝑋𝑛 ) represents the measurement of the state of the subset of neighbouring variables,
𝑋𝑛 , to the factor, and the error term 𝜖 ∼ 𝒩(0, Σ𝑛 ) is white noise. Thus, ﬁnding the residual 𝑟
between the measurement and the model, as seen in (31)[6], reveals propagates the Gaussian
nature of the model to the residual.
𝑟 = 𝑑 − 𝑚(𝑋𝑛 ) ∼ 𝒩(0, Σ𝑛 )

(31)

With this, looking at Figure 1, the Moments Form can be rewritten with the measurement,
𝑚(𝑥), and the model 𝑑 (32)[6]:
16

06-05-2024

Chapter 2: Background ― 2.7 Non-Linearities

1
𝐸(𝑋𝑛 ) = (𝑚(𝑋𝑛 ) − 𝑑)⊤ Σ𝑛 −1 (𝑚(𝑋𝑛 ) − 𝑑)
2

(32)

In case of a linear factor, the measurement function is quadratic and can be written as
𝑚(𝑋𝑛 ) = 𝐉𝑋𝑛 + 𝑐, where 𝐉 is the jacobian. This allows us to rearrange the energy onto
Canonical Form (33)[6]:
1
𝐸(𝑋𝑛 ) = 𝑋𝑛⊤ Λ𝑋𝑛 − 𝜂⊤ 𝑋𝑛
2

, where 𝜂 = 𝐉⊤ Σ𝑛 −1 (𝑑 − 𝑐) and Λ = 𝐉⊤ Σ𝑛 −1 𝐉 (33)

However, in case of a non-linearity in 𝑚, the energy is also not quadratic in 𝑋𝑛 , which in turn
means that the factor is not Gaussian. To achieve a Gaussian distribution for the factor in this
case, it is necessary to linearise around a current estimate 𝑋0 , which is from here called the
linearisation point. This linearisation takes place by (34)[6]:
𝑚(𝑋𝑛 ) = 𝑚(𝑋0 ) + 𝐉(𝑋𝑛 − 𝑋0 )

(34)

As such, we end up with a linearised factor on the form (35)[6], which ends up with a Gaussian
approximation of the true non-linear distribution:
𝑐 = 𝑚(𝑋𝑛 ) − 𝐉𝑋𝑛

(35)

The underlying potential non-linearities of factors is exempliﬁed in Example 4, and visualised
in Figure 5.

Example 4
Jens: make this example with similar ﬁgure as to [6]

Figure 5. A non-linear factor is visualised, where the measurement function 𝑚(𝑋𝑛 ) is non-linear. The linearisation point 𝑙0 is shown, and the robot’s position . The non-linear true distribution is visualised as a grey
contour plot underneat the linearised gaussian distribution on top.

17

Chapter 2: Background ― 2.7 Non-Linearities

2.8

06-05-2024

Rapidly Exploring Random Trees

Rapidly-exploring Random Tree (RRT) is a sampling-based path planning algorithm, introduced by Steven M. LaValle in 1998[13]. The algorithm incrementally builds a tree of nodes,
each node a speciﬁc step length, 𝑠, from the last. The tree is built by randomly sampling a
point in the conﬁguration space, and then extending the tree towards that point with 𝑠. See
the entire algorithm in Algorithm 1.[13,14] Example 5 goes through an contextual example of
the algorithm.

Algorithm 1: The RRT Algorithm
Input: 𝑥start , 𝑥goal , 𝑠, 𝑁 , 𝑔tolerance
𝑉 ← {𝑥start }
𝐸←∅
for 𝑖 = 1, …, 𝑁 do
𝑥random ← SampleRandomPoint()
𝑥nearest ← NearestNeighbor(𝐺 = (𝑉 , 𝐸), 𝑥random )
𝑥new ← Steer(𝑠, 𝑥nearest , 𝑥random )
if CollisionFree(𝑥nearest , 𝑥new ) then
𝑉 ← 𝑉 ∪ 𝑥new
𝐸 ← 𝐸 ∪ {(𝑥nearest , 𝑥new )}
else
continue
end
if WithinGoalTolerance(𝑔tolerance , 𝑥new , 𝑥goal )
∧ CollisionFree(𝑥new , 𝑥goal ) then
𝑉 ← 𝑉 ∪ 𝑥goal
𝐸 ← 𝐸 ∪ {(𝑥new , 𝑥goal )}
break
end
end
Output: 𝐺 = (𝑉 , 𝐸)

18

06-05-2024

2.8.1

Chapter 2: Background ― 2.8 Rapidly Exploring Random Trees

RRT Functions

This section provides a mathematical description of the functions used in the RRT algorithm; functions SampleRandomPoint , NearestNeighbor , Steer , CollisionFree , and
WithinGoalTolerance . As in Algorithm 1, the RRT tree consists of vertices, 𝑉 , and edges,
𝐸; together composing a graph, 𝐺 = (𝑉 , 𝐸). These denotations are used in the following descriptions.[15]

2.8.1.1 SampleRandomPoint() -> x
This functions takes no arguments, and returns a random point, 𝑥, in the conﬁguration space.
Most commonly this is done by drawing from a uniform distribution. Say that 𝜔 is an element
in the set of all possible states in the conﬁguration space Ω, where ∀𝜔 ∈ Ω equation (36) holds.
[15]
SampleRandomPoint : 𝜔 ↦ {SampleRandomPoint𝑖 (𝜔)}

𝑖∈ℕ0

⊂𝒳

(36)

That is; the set of all randomly sampled points, 𝒳rand , which is the result of the above mapping, is a subset of the conﬁguration space, 𝒳.

2.8.1.2 NearestNeighbor(G, x) -> v
Finds the nearest node 𝑣 ∈ 𝑉 ⊂ 𝒳 in the tree to a given point. Takes in the graph, 𝐺 = (𝑉 , 𝐸),
and a point, 𝑥 ∈ 𝒳, see (37). This notion could be further speciﬁed with a distance metric,
such as the Euclidean distance, as seen in (38), which return the node 𝑣 ∈ 𝑉 that minimizes
the distance, ‖𝑥 − 𝑣‖, between the new point 𝑥 and an existing node 𝑣.[15]
NearestNeighbor : (𝐺, 𝑥) ↦ 𝑣 ∈ 𝑉

(37)

NearestNeighbor(𝐺 = (𝑉 , 𝐸), 𝑥) = argmin𝑣∈𝑉 ‖𝑥 − 𝑣‖

(38)

2.8.1.3 Steer(x, y, s) -> v
Creates a new node at a speciﬁc distance from the nearest node towards a given point. Takes
in two points 𝑥, 𝑦 ∈ 𝒳, and a step length 𝑠 ∈ ℝ+ ,. The new node 𝑣 is created by moving 𝑠
distance from 𝑥 towards 𝑦. This way equation (39) returns a point 𝑣 ∈ 𝒳 such that 𝑣 is closer
to 𝑦 than 𝑥, which will either be 𝑠 closer, or if the randomly sampled point 𝑦 is within 𝑠 distance from 𝑥 to begin with, 𝑣 will be at 𝑦. As such the inequality ‖𝑧 − 𝑦‖ ≥ 𝑠 holds.[15]
Steer : (𝑥, 𝑦, 𝑠) ↦ 𝑣 ∈ 𝒳

(39)

2.8.1.4 CollisionFree(x, y) -> p
Checks if the path between two nodes is collision-free. Takes in two points 𝑥, 𝑦 ∈ 𝒳, and
returns a boolean, 𝑝 ∈ {⊤, ⊥}. The returned values, 𝑝, says something about whether the addition of node 𝑦 ∈ 𝒳 into the RRT tree is valid, given a proposed edge to the node 𝑥 ∈ 𝑉 .
Typically the validity notion depends on whether the path from 𝑥 to 𝑦 is collision-free, hence
the function’s name, but could include any other arbitrary constraints.[15]
19

Chapter 2: Background ― 2.8 Rapidly Exploring Random Trees

06-05-2024

2.8.1.5 WithinGoalTolerance(t, x, g) -> p
Checks if a node is within the goal tolerance distance from the goal. As such the the functions
takes in the distance tolerance 𝑡, a node 𝑥 ∈ 𝑉 , and the goal state 𝑔 ∈ 𝒳. The function returns
a boolean, 𝑝 ∈ {⊤, ⊥}, that tells us whether 𝑥 is within a euclidean distance 𝑡 from the goal
state 𝑔. See (40) for the mathematical representation.[15]
WithinGoalTolerance : (𝑡, 𝑣, 𝑔) ↦ 𝑝 ∈ {⊤, ⊥}

(40)

Example 5: Contextual RRT Application
Scenario: Let’s look at an example, where the possible state space is two-dimensional euclidean space. A Robot wants to go from 2D position 𝑥𝐴 to 𝑥𝐵
Input: In Algorithm 1, the input is outlined to be a starting position, 𝑥start , a goal position
𝑥goal , a step length 𝑠, a maximum number of iterations, 𝑁 , and lastly, a goal tolerance,
𝑔tolerance .
Output: At the end of algorithm execution, the resulting graph is outputted as the combination of; 𝑉 , the set of vertices, and 𝐸, the set of edges.
Execution:
1. 𝑉 is initialised to contain the initial position of the robot 𝑥start = 𝑥𝐴 , thus the set {𝑥𝐴 }.
𝐸 is initialised to be empty.
2. Enter a for loop, that will maximally run 𝑁 times, but will break early if the goal is
reached.
Each iteration:
2.1. A random point, 𝑥random , is sampled from the conﬁguration space, by calling the
sampling function SampleRandomPoint().
2.2. The nearest existing node in the tree, 𝑥nearest , is found by NearestNeighbor(𝐺 =
(𝑉 , 𝐸), 𝑥random ).
2.3. Thereafter, a new node, 𝑥new , is created by making a new node 𝑠 distance from
𝑥nearest towards 𝑥random in the call to Steer(𝑠, 𝑥nearest , 𝑥random ).
Checks:
2.1. Only if the path from 𝑥nearest to 𝑥new is collision-free, the new node is added to the
tree. Otherwise, continue to the next iteration.
2.2. If the node is added to the tree, and it is within 𝑔tolerance distance from 𝑥goal , and
the path from 𝑥new to 𝑥goal is collision-free, the goal is added to the tree, and the
loop is broken.

20

06-05-2024

2.9

Chapter 2: Background ― 2.8 Rapidly Exploring Random Trees

Optimal Rapidly Exploring Random Trees

Optimal Rapidly-exploring Random Tree (RRT*) is an extension of the RRT algorithm, which
was introduced in 2011 by Sertac Karaman and Emilio Frazzoli in their paper Sampling-based
Algorithms for Optimal Motion Planning[14]. With only a couple of modiﬁcations to RRT, the
algorithm is able to reach asymptotic optimality, where the original algorithm makes no such
promises. The modiﬁcations are explained below:
M-1: Cost Function: The ﬁrst modiﬁcation is the introduction of a cost function, 𝑐(𝑣), for
each node, 𝑣 ∈ 𝑉 . The cost function outputs the length of the shortest path from the
start node to the node 𝑣. This modiﬁcation encodes an optimisable metric for each
branch, which enables the next modiﬁcation, M-2 , to take place.
M-2: Rewiring: The second modiﬁcation is the introduction of a neighbourhood radius, 𝑟 ∈
ℝ+ , around each newly created node, which is used to search for nodes that can be
reached with a lower cost.
As such every time a new node is created, there is a possibility that other nodes
within that radius, will have a lower cost if they were to be connected to the new node.
Thus, comparing the nodes’ old cost, and the cost they would have in case we connect
them to the newly created node, determines whether to rewire or not.

Algorithm 2: The RRT* Algorithm
Input: 𝑥start , 𝑥goal , 𝑠, 𝑁 , 𝑔tolerance
𝑉 = {𝑥start }
𝐸=∅
for 𝑖 = 1, …, 𝑛 do
𝑥rand ← Sample()
𝑥nearest ← Nearest(𝑉 , 𝐸, 𝑥rand )
𝑥new ← Steer(𝑥nearest , 𝑥rand )
if ObstacleFree(𝑥nearest , 𝑥new ) then
𝑉near ← Neighbourhood(𝑉 , 𝐸, 𝑥new , 𝑟)
𝑉 ← 𝑉 ∪ {𝑥new }
𝑐nearest ← cost(𝑥nearest ) + 𝑐(Line(𝑥nearest , 𝑥new ))
𝑥min ← MinCostConnection(𝑉near , 𝑥new , 𝑥nearest , 𝑐nearest )
𝐸 ← 𝐸 ∪ {[𝑥min , 𝑥new ]}
Rewire(𝑉near , 𝑥new )
end
if WithinGoalTolerance(𝑔tolerance , 𝑥new , 𝑥goal )
∧ CollisionFree(𝑥new , 𝑥goal ) then

21

Chapter 2: Background ― 2.9 Optimal Rapidly Exploring Random Trees

06-05-2024

𝑉 ← 𝑉 ∪ {𝑥goal }
𝐸 ← 𝐸 ∪ {[𝑥new , 𝑥goal ]}
break
end
end
Output: 𝐺 = (𝑉 , 𝐸)

With the modiﬁcations made, the RRT* algorithm is shown in Algorithm 2[15]. Two important
blocks of the algorithm has been sectioned out in sub-algorithms Algorithm 3 and 4, which
are described along side the other new functions of RRT* under Section 2.9.1. The main parts
of the algorithm are visualised in Figure 6 as three steps:
Step 1: A new point has been sampled, deemed collision-free, and thus node 𝑣new can be
added to the tree. But ﬁrst, we need to ﬁnd which existing node to connect to. Here,
𝑣nearest is chosen by the MinCostConnection algorithm, as it is the node that minimizes the total cost from the root to 𝑣new , within the step-length radius 𝑠.
Step 2: In preparation, rewiring candidates will be found, by looking at all nodes in the
tree, that are withing a certain reqiring radius, 𝑟, from 𝑣new . This is done by the
Neighbourhood function, which returns the set 𝑉near = {𝑛1 , 𝑛2 , …, 𝑛𝑛 }.
Step 3: This step is where the rewiring takes place. By looking at the nodes in 𝑉near , we can
compute each node’s cost, 𝑐new with equation (41)
Cost(𝑣new ) + 𝑐(Line(𝑛𝑖 , 𝑣new ))

(41)

as if 𝑣new were its parent. Denote the costs 𝐶near = {𝑐1 , 𝑐2 , …, 𝑐𝑛 }. Now for each
node 𝑛𝑖 ∈ 𝑉near , check if 𝑐𝑖 < 𝑐new , and if so, rewire the connection to make 𝑣new the
parent of 𝑛𝑖 .

Figure 6. The RRT* algorithm drawn out in 3 steps. Firstly, a new node is sampled and added to the tree, where the
cost is lowest, looking in a radius of 𝑠 . Then nodes within a neighbourhood 𝑟 , are then rewired if their cost would
be lower by doing so.

2.9.1

RRT* Functions

Here the functions used in the RRT* algorithm are described in Algorithm 2. Functions from
base-RRT are not repeated here, as no change is made to them. The new functions are;
MinCostConnection , Rewire , Cost , Neighbourhood , Parent , and Line .

22

06-05-2024

Chapter 2: Background ― 2.9 Optimal Rapidly Exploring Random Trees

2.9.1.1 MinCostConnection(V_near, v_new, v_init, c_init) -> v
This function is a main part of the RRT* modiﬁcation, as it attached the new node 𝑣new , not
to the node nearest to the randomly sampled point in 𝒳, but to the node that minimizes the
cost from the root to 𝑣new . This happens by looking at all nodes in a neighbourhood 𝑉near of
radius 𝑟 from 𝑣new , and then ﬁnding the node that minimizes the cost. To begin with the initial
node 𝑣init and its cost 𝑐init is passed to the function as the initial comparison point. The initial
comparison point is typically the nearest node in the tree, that would have been the parent
for 𝑣new in RRT. The function’s operation is described in Algorithm 3.

Algorithm 3: Finding the Minimum Cost Connection
Input: 𝑉near , 𝑥new , 𝑥nearest , 𝑐nearest
𝑥min ← 𝑥nearest
𝑐min ← 𝑐nearest
for 𝑥near ∈ 𝑋near do
𝑐near ← Cost(𝑥near ) + 𝑐(Line(𝑥near , 𝑥new ))
if CollisionFree(𝑥near , 𝑥new ) ∧ 𝑐near < 𝑐min then
𝑥min ← 𝑥near
𝑐min ← Cost(𝑥near ) + 𝑐(Line(𝑥near , 𝑥new ))
end
end
Ouput: 𝑥min

2.9.1.2 Rewire(V_near, v_new)
The rewiring function is the second part of the RRT* optimisation steps, which changes previously established connections in the tree. The function uses the neighbourhood 𝑉near of nodes
in radius 𝑟 around 𝑣new . For each 𝑛𝑖 ∈ 𝑉near , if the cost of 𝑛𝑖 with 𝑣new as parent is lower
than the previously established cost for 𝑛𝑖 , the tree is rewired. The function is described in
Algorithm 4.

Algorithm 4: Rewiring
Input: 𝑉near , 𝑥new
for 𝑥near ∈ 𝑉near do
𝑐near ← Cost(𝑥new ) + 𝑐(Line(𝑥new , 𝑥near ))
if CollisionFree(𝑥new , 𝑥near ) ∧ 𝑐near < Cost(𝑥near ) then
𝑥parent ← Parent(𝑥near )
𝐸 ← 𝐸 \ {[𝑥parent , 𝑥near ]}
𝐸 ← 𝐸 ∪ {[𝑥new , 𝑥near ]}
end

23

Chapter 2: Background ― 2.9 Optimal Rapidly Exploring Random Trees

06-05-2024

end
Output: None

2.9.1.3 Cost(v) -> c
This function is used in Algorithm 2, 3, and 4 to access the cost 𝑐 of a node 𝑣 ∈ 𝑉 . Typically
the cost is a distance, and as such; the sum of the Euclidean distances between all nodes if one
were to walk all the way back to the root node from 𝑣. Thus a mapping from a node 𝑣 ∈ 𝑉 to
a cost 𝑐 ∈ ℝ+ as shown in (42).
Cost : 𝑣 ↦ 𝑐 ∈ ℝ+

(42)

2.9.1.4 Neighbourhood(V, E, x, r) -> V_near
A more complex function, which returns a the set of all nodes in 𝑉 that are within a radius 𝑟
from a potential new node 𝑥 ∈ 𝒳. If the conﬁguration space is 𝒳 = ℝ2 , then the neighbourhood 𝑉near is a subset of 𝑉 such that ∀𝑣 ∈ 𝑉near , ‖𝑣 − 𝑥‖ ≤ 𝑟. This mapping is described in
(43).
Neighbourhood : (𝑉 , 𝐸, 𝑥, 𝑟) ↦ 𝑉near ⊂ 𝑉

(43)

2.9.1.5 Parent(v) -> p
Semantically denotes access to the parent node 𝑝 ∈ 𝑉 of a node 𝑣 ∈ 𝑉 , see (44).
Parent : 𝑣 ↦ 𝑝 ∈ 𝑉

(44)

2.9.1.6 Line(x, y) -> l
Denotes the idea of the ﬁnding the line segment that is between two nodes 𝑥, 𝑦 ∈ 𝒳. This line
segment expresses the relationship between 𝑥 and 𝑦 in the conﬁguration space 𝒳. It can be
used, as seen in algorithms 2, 3 and 4, to calculate the cost that this line segment provides. This
is done by the function 𝑐(Line(𝑥, 𝑦)), which, in case of a Euclidean conﬁguration space, and
thus cost would express a mapping from two points 𝑥, 𝑦 ∈ 𝒳 to a distance 𝑙 ∈ ℝ+ , see (45).
Line : (𝑥, 𝑦) ↦ 𝑙 ∈ ℝ+

𝑐(Line(𝑥, 𝑦)) = ‖𝑥 − 𝑦‖

24

(45)

3

Methodology

3.1

Study 1: Reproduction

3.1.1

Implementation Language

This subsection describes the choice of implementation programming language used in both
the simulation and the reimplementation of the gbpplanner paper. Motivations for why the
language was chosen is laid out and argued for.
Both the simulation and the gbplanner reimplementation are written in the Rust programming language. Rust is a modern systems programming language

3.1.1.1 Why did we choose Rust?
◆ gbpplanner is implemented in C++. We are both familiar with C++, but have in previous
projects spent a lot
of time ﬁghting its idiosyncrasies
Kristoﬀer: What other alternatives were available? C++ Zig Julia Python, too slow, and lack
of a strong (enforced) type system had us worried about managing the implementation as the
project codebase would grow.
Kristoﬀer: Explain some of the unique beneﬁts of Rust.
◆ borrow checker
◆ ownership
◆ memory safety
◆ concurrency
◆ Performance
◆ Rich type system
◆ Rich library ecosystem
◆ Helpful error message
◆ Error handling, errors as values, no exceptions
◆ Exceptional tooling,

25

Chapter 3: Methodology ― 3.1 Study 1: Reproduction

06-05-2024

Kristoﬀer: Explain some of the drawbacks of Rust.
◆ Complexity, can be hard to learn
◆ Some design pattern/implementations are not trivial to implement. Especially self referential/recursive structures like graphs.
◆ Slow compilation
◆ Not as many very established libraries, like C++ which has been around for a lot longer

3.1.2

Signed Distance Field

3.1.3

Architecture

This section presents the architectural patterns used in the design of the simulation. First by
presenting the major architectural paradigm used, the ECS paradigm. What it is, how it works
and why it was chosen. Second how it results in changes compared to the original implementation of the gbplanner algorithm.

3.1.3.1 Entity Component System

Kristoffer
explain
what
is
meant by
data oriented programing

Entity Component System (ECS) is an architectural software design pattern speciﬁcally designed for data oriented programming . At the heart of it are three complementary concepts,
from which its name comes from: entities, components and systems:
Entity A collection of components with a unique id. Every object in the ECS world is an
entity. Most often the id a single unsigned integer.
Component Data scoped to a single piece of functionality. For example position, velocity,
rigid body, a timer etc.
System Functions that operate on the data by querying the ECS world for entities and components and updating them.
It if diﬀerent from traditional Object Oriented Programming (OOP) based ways of modelling
At ﬁrst glance this representation/organisazation
that is designed to fully utilize modern computer hardware
memory hierarchies
Cache Locality
Kristoﬀer: point out how it is diﬀerent compared to traditional game engines/simulators
like Unity, Unreal, autodesk Isaac Sim
Kristoﬀer: create ﬁgure explaining why ECS is cache friendly
Kristoﬀer: Make a remark about the similarities with relational databases and query synteax like SQL. Also point out how the data is stored diﬀerently, to handle concern about cache
friendlyness
data oriented design vs object oriented design
26

06-05-2024

Chapter 3: Methodology ― 3.1 Study 1: Reproduction

The ECS architecture is not limited to game engines and simulations.
is versatile
Kristoﬀer: what are its drawbacks?
Kristoﬀer: cite ecs papers
Kristoﬀer: create ﬁgure explaining ECS data store
entity similar to a primary key in a relational database
Kristoﬀer: clarify that the data representation used by bevy is not one to one of the table
example
Entity (ID)

Transform

𝑎

✔

𝑎+1

✔

✔

✔ [0.2, 0.8]

𝑎+2

✔

✔

✔ [−0.5, 0.0]

𝑎+3

✔

✔

𝑎+4

✔

✔

Robot

Camera

Velocity2d

✔

✔ [0.0, 1.0]

Obstacle

…

…
𝑎+𝑛

✔

✔

✔ [1.0, 0.0]

Table 1. Structural layout of an ECS data store. Conceptually it is analogous to a table in relational database. ✔ in a
component column denotes that the entity has an instance of that component type e.g. entity 𝑎 + 2 has components:
{ Transform , Robot , Velocity2d }
fn move_robots(mut query: Query<(&mut Transform, &Velocity2d), With<Robot>>) {
for (mut transform, velocity) in &mut query {
...
}
}
Entity (ID)

Transform

𝑎

✔

𝑎+1

✔

✔

✔ [0.2, 0.8]

𝑎+2

✔

✔

✔ [−0.5, 0.0]

𝑎+3

✔

✔

𝑎+4

✔

✔

Robot

Camera

Velocity2d

✔

✔ [0.0, 1.0]

…
𝑎+𝑛

✔

✔

✔ [1.0, 0.0]

The With<Robot> is a
where

27

Obstacle

…

Chapter 3: Methodology ― 3.1 Study 1: Reproduction

3.1.4

Kristoffer ﬁnd
citation
for
this
statement

06-05-2024

Graph Representation

Kristoﬀer: talk about how our graph representation is diﬀerent from theirs. Ours is more
faithful to how robots would represent the other robots in the environment compared to
theirs, since they use bidirectional std::shared_ptr , which is not useable in a scenario
where the algorithm run on diﬀerent computer hosts, or just computer processes on the same
host.
There are several diﬀerent ways of representing graph structures in computer memory.
Each with its own advantages and disadvantages. As explained in Section 2.4, the factorgraph
structure is a bipartite graph with undirected edges. Such a graph structure enforces little to
no constraints on what kind of memory representation are possible to use . In the original
work by Patwardhan et al.[5] a cyclic reference/pointer structure is used. They represent the
graph with a C++ class called FactorGraph , which each robot instance inherit from. Variable
and factor nodes are stored in two separate vectors; factors_ and variables_ , as shown in
the top of Code Reference 1.
Kristoﬀer: Add static link to github, and a line range
// defined in `inc/gbp/FactorGraph.h`
class FactorGraph {
public:
std::vector<std::shared_ptr<Factor>> factors_;
std::vector<std::shared_ptr<Variable>> variables_;
// ...
};
// defined in `inc/gbp/Factor.h`
class Factor {
public:
// Vector of pointers to the connected variables. Order of variables matters
std::vector<std::shared_ptr<Variable>> variables_{};
// ...
};
// defined in `inc/gbp/GBPCore.h`
class Key {
public:
int robot_id_;
int node_id_;
};
// defined in `inc/gbp/Variable.h`
class Variable {
public:
// Map of factors connected to the variable, accessed by their key
std::map<Key, std::shared_ptr<Factor>> factors_{};
// ...
}

Code Reference 1. FactorGraph class declaration in inc/gbp/FactorGraph.h Kristoﬀer: write proper caption

Edges between variable and factors are not stored as separate index values, but are instead
implicitly stored by having each factor storing a std::shared_ptr<Variable> to every variable it is connected to. Complementary every variable stores a std::shared_ptr<Factor> to
every factor it is connected to. This kind of structure is advantageous in that it easy access
the neighbours of node, given only a handle to the node. For example to send messages to

28

06-05-2024

Kristoffer mention that
their representation does
not map
well/reﬂect how
each robot would
represent
connections to
external
robots
when running on
diﬀerent
hosts

Chapter 3: Methodology ― 3.1 Study 1: Reproduction

the neighbours of a node by directly invoking methods on the receiving node directly. This
structural pattern however is diﬃcult to implement and discouraged in Rust due to its unique
language feature for managing memory; the ownership model. This model is comprised of
three rules, that together ensures memory safety and prevents memory issues like use after
free and memory leaks[16].
1. Each value has exactly one owner
2. There can only be one owner at a time.
3. When the owner goes out of scope, the value is dropped1.
One limitation of this system is that data structures with interior bidirectional references like
the gbpplanners factor graph representation are diﬃcult to express, since there is no conceptual single owner of the graph. If a factor and a variable that are connected, both share a
reference to the memory region of each other, then there is not a well deﬁned concept of who
owns who. Diﬃcult does not mean impossible, and there are ways to express these kinds of
data structures using a Rust speciﬁc design pattern called the Interior Mutability pattern[16].
We decided not to use this pattern and instead work within the intended modelling constructs
of the Rust language. In the reimplementation the graph data structure uniqly owns all the
variable and factor nodes. And nodes in the in the graph store no interior references to nodes
they are connected to. The petgraph library is used for the actual graph datastructure. petgraph
is a versatile and performant graph data structure library providing various generic graph
data structures with diﬀerent performance characteristics[17]. To choose which graph representation to use the following requirements were considered. The requirements are ordered
by priority in descending order:
1. Dynamic insertion and deletion of nodes. Robots connect to each others factorgraph when
they both are within the communication radius of each other and both their communication mediums are active/reachable. Likewise they disconnect when they move out of each
others communication or the other one is unreachable. This connection is upheld by the
InterRobot factor, which gets added and removed frequently.
2. Fast node iteration and neighbour search: In order to ensure collision free paths at tolerable
speeds, the GBP algorithm will have to run many times a second. The faster each factorgraph can be traversed the better.
3. The index of a node is valid for the lifetime of the node. Without this a lot of checks have
to be added every time the factorgraph is indexed to get a reference to a node.
petgraph supports the following ﬁve types of graph representation:
Name

Description

Space Com- Backing Ver- Dynamic
plexity
tex Structure

Stable
Indices

Hashable
vertices

Graph

Uses an Adjacency List to
store vertices.

𝑂(|𝐸| + |𝑉 |)

Vec<N>

✔

x

x

StableGraph

Similar
to
Graph , but it

𝑂(|𝐸| + |𝑉 |)

Vec<N>

✔

✔

x

1
In Rust the term “dropped” is the preferred term to communicate that a value is destructed and its memory
deallocated.

29

Chapter 3: Methodology ― 3.1 Study 1: Reproduction

Name

Description

06-05-2024

Space Com- Backing Ver- Dynamic
plexity
tex Structure

Stable
Indices

Hashable
vertices

keeps indices
stable across
removals.
GraphMap

Uses an associative array, but instead of storing vertices sequentially it
uses generated
vertex identiﬁers as keys
into a hash
table, where
the value is
a list of the
vertices’ connected edges.

IndexMap<N,
✔
𝑂(|𝐸| +
Vec<(N,
CompactDirection):>
|𝑉 |) ∗

x

✔

MatrixGraph

Uses an Adjacency Matrix
to store vertices.

𝑂(|𝑉 2 |)

Vec<N>

✔

x

x

CSR

Uses a sparse
adjacency matrix to store
vertices, in the
Compressed
Sparse
Row
(CSR) format.

𝑂(|𝐸| + |𝑉 |)

Vec<N>

✔

x

x

Table 2. Available Graph Representations in the petgraph library. |𝐸| is the number of edges and |𝑉 | is the number
of nodes. The “Backing Node Structure” lists which underlying data structure is used to store the associated of each
vertex. Vec<N> 2 is a growable array where items are placed continuous in memory[18]. IndexMap<N> is a special
hash map structure that uses a hash table for key-value indices, and a growable array of key-value pairs. Allows for
very fast iteration over nodes since their memory are densely stored in memory[19]. The “Dynamic” column labels
if the data structure supports vertices/edges being removed after initialization. The “Hashable vertices” columns list
if the data structure requires that the vertex type must be hashable.

Kristoffer ehh…
maybe

All ﬁve graph representations support dynamic insertion and removal of vertices and edges
after initialization of the graph. So all of them satisfy the ﬁrst requirement. Four out of the ﬁve
graph representations uses a Vec<N> as its underlying container for vertex instances. Vec<N>
are guaranteed to be continuous in memory ensuring fast iteration due to cache locality. At the
same time the relative diﬀerence in iteration speed of using a the GraphMap structure should
not really be noticeable, given that it uses an IndexMap<N> , which in turn uses a Vec<(N, E)>
for its underlying storage of vertices. But it adds the additional constraint that vertices needs
to be hashable, which is impractical given the lack of non-unique immutable ﬁelds of the Node
struct. So all data structures support the second requirement. Only the StableGraph data
structure guarantees stable indices across repeated removal and insertion. Leaving it as the
2

Part of Rust’s standard library

node_index

ﬁeld, but
Option⁇?

30

06-05-2024

Chapter 3: Methodology ― 3.1 Study 1: Reproduction

sole viable choice left that meets all three requirements. Expressed using the petgraph library
the chosen Graph type is deﬁned as:
type IndexSize = u16; // 2^16 - 1 = 65535
pub type NodeIndex = petgraph::stable_graph::NodeIndex<IndexSize>;
pub type Graph = petgraph::stable_graph::StableGraph<Node, (), Undirected, IndexSize>;

Code Reference 2.

Kristoffer ﬁnd
this number

Kristoﬀer: explain () type parameter for edge data
IndexSize is a type parameter for the upperbound of the number of nodes the graph can
hold. In our experimentsSection 4 no individual factorgraph ever held more more than ~, so
a bound of 216 − 1 was suﬃcient, and takes up less space than 232 − 1.
In terms of space complexity all ﬁve candidates are close to equivalent, with four of them
using 𝑂(|𝑉 | + |𝐸|) space, and the MatrixGraph using 𝑂(|𝑉 |2 ). Lack of suﬃcient/enough
memory were not deemed and issue for the simulation. To support this claim let:
𝐵(𝑇 ) = stack allocation of T in bytes

(46)

The standard library function std::mem::size_of::<T>() is used to calculate 𝐵(𝑇 )[18].
Then the size of a VariableNode and a FactorNode is:
𝐵(Variable) = 392 bytes

(47)

𝐵(Factor) = 408 bytes

(48)

A Node is modelled as a tagged union of a VariableNode and FactorNode so the size of a
node is, the size of the largest of the two variants, plus 8 bytes to store the tag[16]:
𝐵(𝑣𝑖 ) = max(𝐵(Variable), 𝐵(Factor)) + 8 bytes

(49)

No data is associated with an edge so the size of an edge is:
𝐵(𝑒𝑖 ) = 0 bytes

(50)

𝐵(FactorGraph) = 200 bytes

(51)

An empty FactorGraph takes up:

Kristoﬀer: explain functions and meaning of variable names
𝑁obstacle (#𝑉 ) = #𝑉 − 2

(52)

𝑁dynamic (#𝑉 ) = #𝑉 − 1

(53)

𝑁interrobot (#𝑉 , #𝐶) = #𝑉 × #𝐶

(54)

𝑁factors (#𝑉 , #𝐶) = 𝑁obstacle (#𝑉 ) + 𝑁dynamic (#𝑉 ) + 𝑁interrobot (#𝑉 , #𝐶)

(55)

With a simulation of #𝑅 robots and #𝑉 variables, with each robot having #𝐶 connections,
then the size is:
𝐵(Simulation(𝑅, 𝑉 , 𝐶)) = 𝑅 × (𝐵(FactorGraph) + (𝑉 + 𝑁factors (𝑉 , 𝐶)) × 𝐵(Node))
(56)
31

Chapter 3: Methodology ― 3.1 Study 1: Reproduction

06-05-2024

With 𝑅 = 20, 𝑉 = 10, 𝐶 = 10 the size is:
𝐵(Simulation(20, 10, 10)) = 1060640 bytes = 1.011505126953125 MiB

(57)

1.011505126953125 MiB is not a lot of memory for a modern computer. This of course, only
accounts for the stack allocated memory of each structure. For heap allocated structures like
dynamically sized matrices this only accounts for the heap pointer to the data and the length
of the allocated buﬀer, and not the size of the buﬀer. With 4 Degrees of Freedom (DOF) a
conservative estimate can be made by generalizing the heap allocation size of each node to be
the largest heap allocation of the possible node variants. Let
𝐻(𝑇 ) = heap allocation of T in bytes

(58)

𝐻inbox (𝑇 , 𝐶) = heap allocation of T's inbox with C connections in bytes

(59)

𝑉current
𝑉horizon

𝐻
𝐻inbox (𝐶)
480 192 × (1 + 𝐶)
480 192 × (1 + 𝐶)

𝐻total (𝐶)
672 + 192𝐶
672 + 192𝐶

𝑉in-between 480 192 × (3 + 𝐶) 1056 + 192𝐶
192
864
𝐹dynamic 672
𝐹interrobot 416

384

800

[20]
Too summarize memory consumption should not be a limiting factor of simulating the system.
Not too many nodes in the graph, so we did not spend time benchmarking the various
backing memory models.
Since the memory required for each graph is not very high, we can aﬀord to use more space
for additional indices arrays
In addition to storing the graph itself each factorgraph use additional memory to store arrays of node indices of each node kind, to speed iteration
Adjacency matrix Adjacency list
one requirement the chosen graph model would need to satisfy, is the ability to update the
graph structure over time,
supported fast repeated iteration
trade some speed for edge lookup time, for having indices not being invalidated
consider insertion/deletion of nodes and edges as inﬂuencers of the graph data structure. j
using dedicated indices arrays for factors and variables to speed up iteration for queries
only requiring access to the nodes or variables.
Jens:

3.1.5

Asynchronous Message Passing

32

06-05-2024

Chapter 3: Methodology ― 3.1 Study 1: Reproduction

Kristoﬀer: show screenshots side by side of diﬀerent elements of the simulation from theirs
and ours, e.g. visualisation of the factorgraph, or how we added visualisation of each variables
gaussian uncertainty
use this to argue on a non-measurable level why our implementation has is similar to
theirs / has been reproduced
Kristoﬀer: List out all the conﬁguration parameters both the algorithm exposes and the
sim. Which one is identical to gbpplanner, and what values are sensible to use as defaults

Figure

7.
Variable
timesteps,
Kristoﬀer: explain ﬁgure, and review design, use same variable names as rest of document

Kristoﬀer: Explain the marginalization step/ algorithm They do explain it at ALL in the
paper so, we need another citation for it Maybe make a ﬁgure, or some colorful equations to
explain the step/slices
Kristoﬀer: Explain how factors are abstracted using the Factor trait. How our implementation uses Composition instead of inheritance in C++
◆ What pros/cons does this bring?
trait Factor {
/// Name of the factor. Useful for debugging purposes
fn name(&self) -> &'static str;
/// Number of neighbours this factor expects
fn neighbours(&self) -> usize;
/// Whether the factor is linear or non-linear
fn linear(&self) -> bool;
/// The delta for the jacobian calculation
fn jacobian_delta(&self) -> f64;
/// The jacobian of the factor
fn jacobian(&self, state: &FactorState, x: &Vector<f64>) -> Cow<'_,
Matrix<f64>>;
/// Measurement function
fn measure(&self, state: &FactorState, x: &Vector<f64>) -> Vector<f64>;
/// First order jacobian (provided method)
fn first_order_jacobian(&self, state: &FactorState, x: Vector<f64>) ->
Matrix<f64> { ... }
}

3.2

Study 2: Improvements

33

Chapter 3: Methodology ― 3.2 Study 2: Improvements

3.2.1

06-05-2024

Iteration Schedules

Scheduling, order in which we call internal and external iteration
Kristoﬀer: Talk about how the paper make a distuinguishon about that internal and external iteration can be varied in amount per algorithm step. But does not mention how they are
ordered relative to each other i.e. what schedule they use.

Internal 10
Internal 5
Interleave Evenly

1

2

3

4

5

6

7

8

9

10

7

8

9

10

7

8

9

10

7

8

9

10

7

8

9

10

Soon as Possible

1

2

3

4

5

6

Late as Possible

1

2

3

4

5

6

Half at the beginning, Half at the end

1

2

3

4

5

6

Centered

1

3.3

2

3

4

5

6

Study 3: Extension

34

06-05-2024

3.3.1

Chapter 3: Methodology ― 3.3 Study 3: Extension

Global Planning

Global planning has been made as an extension to the original GBP Planner software developed by Todo: cite . The original algorithm works very well on a local level, and lacks a global
overview of how to get from A to B as seen in Figure 8.
An example of the RRT algorithm in action can be seen in Figure 8.
Jens
more
about the
collider,
and the
environment representation.

Figure 8. RRT algorithm and environment integration.

3.3.1.1 Path Adherence
Jens: Approach 1: Simply perform RRT*, and use the resulting points as waypoints.
Jens: Approach 2: Use the RRT* algorithm to generate a path, then track the robots along it
with pose factors (traction factors).

35

Chapter 3: Methodology

3.4

06-05-2024

Study 4: Tooling & Design

Kristoﬀer: talk about design of diﬀerent conﬁg format design, especially formation and
environment

show examples of them, and how they allow for ﬂexibly and declaratively deﬁne new simulation scenarios
formations:
- repeat-every:
secs: 8
nanos: 0
delay:
secs: 2
nanos: 0
robots: 1
initial-position:
shape: !line-segment
- x: 0.45
y: 0.0
- x: 0.55
y: 0.0
placement-strategy: !random
attempts: 1000
waypoints:
- shape: !line-segment
- x: 0.45
y: 1.25
- x: 0.55
y: 1.25
projection-strategy: identity

36

4

Results

4.1

Metrics

To objectively compare our reimplementation with the original GBP Planner, we measure and
compare the same four metrics: distance travelled, makespan, smoothness, and inter robot
collisions[5]:
1. Distance travelled The cumulative distance covered by the robot until it reaches its destination. Eﬀective trajectories aim to minimize this measure.
2. Makespan The overall duration for all robots to achieve their objectives. A collaborative
system of numerous robots should strive to reduce this measure.
3. Smoothness Continuous smooth trajectories are required in most cases, in order to be
realisable for the dynamics model of the robot and other real world constraints. .
Kristoffer like
what,
torque,
friction?

Kristoffer higher

Smoothness is a geometric property of the path traversed.
Kristoﬀer: use same citation as them
dimensionless metric
This metric aims to quantify the smoothness of the robot’s trajectories.
Δ

𝐿𝐷𝐽 = − ln(

2
(𝑡final − 𝑡start )3 𝑡final ¨
|𝑣
(𝑡)|
∫
𝑑𝑡)
2
𝑣max
𝑡

(60)

start

is better?

where 𝑡 ∈ [𝑡start , 𝑡final ] is the time interval the metric is measured over. 𝑣(𝑡) is the velocity of
a robot at time 𝑡, and 𝑣max is the maximum velocity along the trajectory.
4. Inter robot Collisions Number of collisions between robots. The physical size of each robot is represented by a bounding circle. A collision between two robots happen when their
circles intersects.
In addition to the metrics used by by Patwardhan et al.[5] we also consider the following
metrics:
5. Root Mean Squared Error (RMSE)

37

Chapter 4: Results

4.2

06-05-2024

Scenarios

The performance of the reimplementation is evaluated in four diﬀerent scenarios; Circle,
Clear Circle, and Junction. These scenarios adhere to the original paper’s[5] experiments.
S-1 Circle: This environment presents 6 small obstacles in the middle of the environment. 30
robots are placed in a circle, centered on the environment with radius 𝑟 = 50𝑚 around
the obstacles. The robots are tasked with reaching the opposite side of the circle.
S-2 Clear Circle: This environment is similar to the Circle scenario, but without any obstacles. Again 30 robots are placed in a circle, centered with 𝑟 = 50𝑚, and are tasked with
reaching the opposite side of the circle.
S-3 Junction: This environment is much more constrained, only with two roads; a vertical
and horizontal one, centered in their cross-axis. Thus creating a simple crossroads the
very center of the environment.
Speciﬁc details and parameters for each environment are presented in the following sections
4.2.1, 4.2.2, and 4.2.3. Additionally, environment visualisations are provided in ﬁgures 9, 10,
and 11.

4.2.1

Kristoffer is this
t0 ?

Circle
Param

Value

Δ𝑇

0.1𝑠

𝑀𝑅

10

𝑀𝐼

50

𝜎𝑑

1𝑚

𝜎𝑝

1 × 10−15

𝜎𝑟

0.005

𝜎𝑜

0.005

𝐶radius

50𝑚

𝑟𝑅
Initial speed

randomly sampled from 𝒰(2, 3)𝑚

𝑁𝑅

{5, 10, 15, 20, 25, 30, 35, 40, 45, 50}

15 𝑚/𝑠

𝑀𝐼 Internal GBP messages
𝑀𝑅 External inter-robot GBP messages
𝑁𝑅 Number of robots
𝑟𝑅 Robot radius
Figure 9. Circle scenario.

38

06-05-2024

4.2.2

Chapter 4: Results ― 4.2 Scenarios

Clear Circle
Figure 10. Clear Circle scenario.

4.2.3

Junction
Figure 11. Junction scenario.

4.3

Study 1: Reproduction

4.4

Study 2: Improvements

4.5

Study 3: Extension

4.6

Study 4: Tooling & Design

39

5

Discussion
Kristoffer discuss what
would
need to
change
from our
simulation to a
system
working
with real
world robots
◆ Detection of
neighbors

Kristoﬀer: discuss why their implementation has a higher throughput that ours 😔 due to
graph representation. They use OpenMP

5.1

Future Work

5.1.1

Verify simulation results in a Real World setup

5.1.2 Use ray casting instead of sampling an SDF image of the
environment
◆ Use diﬀerent behavior when communication is very bad. Either change the factor graph
by adding
new nodes with diﬀerent policies or a new algorithm all together e.g. the game theory paper
we read at the start.
◆ Extend to 3D world. e.g. can it work with UAVS/drone. What would have to change?
◆ The state space is more complex. As a result the matrices being sent around are larger,
and more computationally costly
◆ What factors would have to change or be updated?
◆ Have other already done something similar
◆ Have the factorgraph be able to change the number of variable nodes during the lifetime
of the graph. I.e. having it dependant on the desired velocity. If we want to accelerate
the robot and have it move faster, it might be better to have more variables/longer future
horizon, while if we do not move very fast, fewer variables could be suﬃcient and less
computationally taxing in that case.

40

06-05-2024

Chapter 5: Discussion ― 5.1 Future Work

◆ Establish a simple network protocol to negotiate and establish the lifecycle of the bidirected
connection between two robots.
◆ How do two robots connect?
◆ How do they communicate?
◆ How do they check the connection is still alive?
◆ How do they reconnect? Is it any diﬀerent from just disconnecting and then reconnecting, or is it necessary to have some kind of stateful protocol? Should be avoided
◆ How do they disconnect?
Kristoﬀer: Experiment with diﬀerent graph representations, e.g. matrix, csr, map based etc.
and test if they match theoretical performance proﬁles derived in the graph representation
section

41

6

Conclusion
Todo: conclusion

42

References
1.

Wisse M, Chiang T-C. WP1 - Competence Centers and Technical Expertise Management.
(2020)

2.

Kiadi M, Villar J R, Tan Q. Synthesized A* Multi-robot Path Planning in an Indoor Smart
Lab Using Distributed Cloud Computing. In: 15th International Conference on Soft Computing Models in Industrial and Environmental Applications (SOCO 2020). Herrero Á, Cambra C, Urda D, Sedano J, Quintián H, Corchado E (Eds.), Springer International Publishing,
Cham, 580–589 (2021)

3.

Liu X, Peters L, Alonso-Mora J. Learning to Play Trajectory Games Against Opponents
with Unknown Objectives [Internet]. (2023). Available from: http://arxiv.org/abs/2211.1
3779

4.

Patwardhan A, Murai R, Davison A J. Distributing Collaborative Multi-Robot Planning
With Gaussian Belief Propagation. IEEE Robotics and Automation Letters [Internet]. 8(2),
552–559 (2023). Available from: https://ieeexplore.ieee.org/document/9976221/

5.

Patwardhan A, Murai R, Davison A J. Distributing Collaborative Multi-Robot Planning
With Gaussian Belief Propagation. IEEE Robotics and Automation Letters. 8(2), 552–559
(2023)

6.

Ortiz J, Evans T, Davison A J. A visual introduction to Gaussian Belief Propagation. arXiv
preprint arXiv:2107.02308. (2021)

7.

Dellaert F, Kaess M. Factor Graphs for Robot Perception. Foundations and Trends in Robotics [Internet]. 6(1–2), 11–13 (2017). Available from: http://www.nowpublishers.com/ar
ticle/Details/ROB-043

8.

Loeliger H-A. An introduction to factor graphs. IEEE Signal Processing Magazine [Internet]. 21(1), 28–41 (2004). Available from: https://ieeexplore.ieee.org/document/1267047

9.

Alevizos P. Factor Graphs: Theory and Applications. (2012)

10. Lecun Y, Chopra S, Hadsell R. A tutorial on energy-based learning. , 34–37 (2006)
11. Murai R, Ortiz J, Saeedi S, Kelly P H, Davison A J. A robot web for distributed manydevice localisation. IEEE Transactions on Robotics. (2023)
12. Eustice R M, Singh H, Leonard J J. Exactly Sparse Delayed-State Filters for View-Based
SLAM. IEEE Transactions on Robotics [Internet]. 22(6), 1100–1114 (2006). Available from:
https://ieeexplore.ieee.org/document/4020357
13. LaValle S. Rapidly-exploring random trees : a new tool for path planning. The annual
research report [Internet]. (1998). Available from: https://www.semanticscholar.org/pape
r/Rapidly-exploring-random-trees-%3A-a-new-tool-for-LaValle/d967d9550f831a8b3f5cb0
0f8835a4c866da60ad

43

Chapter R: ferences

06-05-2024

14. Karaman S, Frazzoli E. Sampling-based Algorithms for Optimal Motion Planning [Internet]. (2011). Available from: http://arxiv.org/abs/1105.1186
15. RRT Star - ERC Handbook [Internet]. . Available from: https://erc-bpgc.github.io/handbo
ok/automation/PathPlanners/Sampling_Based_Algorithms/RRT_Star/
16. The Rust Project Developers. The Rust Programming Language [Internet]. . Available
from: https://doc.rust-lang.org/book/
17. Anton Kochkov, bluss, Agustín Borgna, Bilal Mahmoud. petgraph. (2024)
18. The Rust Project Developers. The Rust Standard Library. (2024)
19. Josh Stone, bluss. indexmap. (2024)
20. Wheatman B, Xu H. Packed Compressed Sparse Row: A Dynamic Graph Representation.
In: 2018 IEEE High Performance extreme Computing Conference (HPEC), 1–7 (2018)

44
